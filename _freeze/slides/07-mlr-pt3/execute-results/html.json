{
  "hash": "9183e4ddfe263272e66dbb534d89f146",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"ANOVA + Geometric interpretation\"\nauthor: \"Prof. Maria Tackett\"\ndate: \"2024-09-17\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— STA 221 - Fall 2024](https://sta221-fa24.netlify.app)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    include-before: [ '<script type=\"text/x-mathjax-config\">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nfilters:\n  - parse-latex\nbibliography: references.bib\n---\n\n\n\n\n\n\n## Announcements\n\n-   Lab 02 due on **Thursday at 11:59pm**\n\n    -   Push work to GitHub repo\n\n    -   Submit final PDF on Gradescope + select all team members + mark pages for each question\n\n-   HW 01 due **Thursday at 11:59pm**\n\n    -   Note submission instructions\n\n## Homework submission {.midi}\n\nIf you write your responses to Exercises 1 - 4 by hand, you will need to combine your written work to the completed PDF for Exercises 5 - 10 before submitting on Gradescope.\n\nInstructions to combine PDFs:\n\n-   Preview (Mac): [support.apple.com/guide/preview/combine-pdfs-prvw43696/mac](https://support.apple.com/guide/preview/combine-pdfs-prvw43696/mac)\n\n-   Adobe (Mac or PC): [helpx.adobe.com/acrobat/using/merging-files-single-pdf.html](https://helpx.adobe.com/acrobat/using/merging-files-single-pdf.html)\n\n    -   Get free access to Adobe Acrobat as a Duke student: [oit.duke.edu/help/articles/kb0030141/](https://oit.duke.edu/help/articles/kb0030141/)\n\n## Latex in this class\n\nFor this class you will need to be able to...\n\n-   Properly write mathematical symbols, e.g., $\\beta_1$ not *B1,* $R^2$ not *R2*\n\n-   Write basic regression equations, e.g., $\\hat{y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2$\n\n-   Write matrix equations: $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$\n\n-   Write hypotheses (we'll start this next week), e.g., $H_0: \\beta = 0$\n\nYou are welcome to but <u>not</u> required to write math proofs using LaTex.\n\n## Topics\n\n-   Compare models using Adjusted $R^2$\n\n-   Introduce the ANOVA table\n\n-   Use a geometric interpretation to find the least squares estimates\n\n## Computing setup\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(viridis) #adjust color palette\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))\n```\n:::\n\n\n\n\n\n\n## Data: Peer-to-peer lender\n\nToday's data is a sample of 50 loans made through a peer-to-peer lending club. The data is in the `loan50` data frame in the **openintro** R package.\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 50 Ã— 4\n   annual_income_th debt_to_income verified_income interest_rate\n              <dbl>          <dbl> <fct>                   <dbl>\n 1             59           0.558  Not Verified            10.9 \n 2             60           1.31   Not Verified             9.92\n 3             75           1.06   Verified                26.3 \n 4             75           0.574  Not Verified             9.92\n 5            254           0.238  Not Verified             9.43\n 6             67           1.08   Source Verified          9.92\n 7             28.8         0.0997 Source Verified         17.1 \n 8             80           0.351  Not Verified             6.08\n 9             34           0.698  Not Verified             7.97\n10             80           0.167  Source Verified         12.6 \n# â„¹ 40 more rows\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Variables\n\n**Predictors**:\n\n::: nonincremental\n-   `annual_income_th`: Annual income (in \\$1000s)\n-   `debt_to_income`: Debt-to-income ratio, i.e. the percentage of a borrower's total debt divided by their total income\n-   `verified_income`: Whether borrower's income source and amount have been verified (`Not Verified`, `Source Verified`, `Verified`)\n:::\n\n**Response**: `interest_rate`: Interest rate for the loan\n\n## Model fit in R\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nint_fit <- lm(interest_rate ~ debt_to_income + verified_income  + annual_income_th, data = loan50)\n\nint_fit2 <- lm(interest_rate ~ debt_to_income + verified_income  + annual_income_th + verified_income * annual_income_th, data = loan50)\n```\n:::\n\n\n\n\n\n\n# Model assessment and comparison\n\n## RMSE & $R^2$\n\n-   **Root mean square error, RMSE**: A measure of the average error (average difference between observed and predicted values of the outcome)\n\n-   **R-squared**, $R^2$ : Percentage of variability in the outcome explained by the regression model\n\n## Comparing models\n\n::: incremental\n-   Though we use $R^2$ to assess the model fit, it is generally unreliable for comparing models with different number of predictors. Why?\n\n    -   $R^2$ will stay the same or increase as we add more variables to the model . Let's show why this is true.\n\n    -   If we only use $R^2$ to choose a best fit model, we will be prone to choose the model with the most predictor variables.\n:::\n\n## Adjusted $R^2$\n\n-   **Adjusted** $R^2$: measure that includes a penalty for unnecessary predictor variables\n-   Similar to $R^2$, it is a measure of the amount of variation in the response that is explained by the regression model\n\n## $R^2$ and Adjusted $R^2$\n\n$$R^2 = \\frac{SSM}{SST} = 1 - \\frac{SSR}{SST}$$\n\n<br>\n\n. . .\n\n$$R^2_{adj} = 1 - \\frac{SSR/(n-p-1)}{SST/(n-1)}$$\n\nwhere\n\n-   $n$ is the number of observations used to fit the model\n\n-   $p$ is the number of terms (not including the intercept) in the model\n\n## Compare models {.midi}\n\nWhich model would you select `int_fit` (main effects only) or `int_fit2` (main effects + interaction) based on...\n\n$R^2$\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(int_fit)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.279854\n```\n\n\n:::\n\n```{.r .cell-code}\nglance(int_fit2)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2963437\n```\n\n\n:::\n:::\n\n\n\n\n\n\n. . .\n\n$Adj. R^2$\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(int_fit)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.215841\n```\n\n\n:::\n\n```{.r .cell-code}\nglance(int_fit2)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1981591\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## ANOVA table {.midi}\n\n\n\n\n\n\n```{=latex}\n\\begin{table}\n\\begin{tabular}{l|l|l|l|l}\n \\hline\n Source & Sum of squares & DF & Mean square & F \\\\\n \\hline\n Model & $\\sum_{i=1}^n(\\hat{y}_i - \\bar{y})^2$ & $p$ & $SSM / p$ & $MSM / MSR$ \\\\ \\hline\n Residual & $\\sum_{i=1}^n(y_i- \\hat{y}_i)^2$ & $n - p - 1$ & $SSR / (n - p - 1)$ & \\\\ \\hline\n Total & $\\sum_{i = 1}^n(y_i - \\bar{y})^2$ & $n - 1$ & &  \\\\\n \\hline\n\\end{tabular}\n\\end{table}\n```\n\n\n\n\n\n<br>\n\n. . .\n\n-   The **degrees of freedom (df)** are the number of independent pieces of information used to calculate a statistic.\n\n-   **Mean square (MS)** is the sum of squares divided by the associated degrees of freedom.\n\n## Using $R^2$ and Adjusted $R^2$\n\n-   Adjusted $R^2$ can be used as a quick assessment to compare the fit of multiple models; however, it should not be the only assessment!\n\n-   Use $R^2$ when describing the relationship between the response and predictor variables\n\n# Geometric interpretation\n\n## Geometry of least squares regression\n\n::: incremental\n-   Let $\\text{Col}(\\mathbf{X})$ be the **column space** of $\\mathbf{X}$: the set all possible linear combinations (span) of the columns of $\\mathbf{X}$\n\n-   The vector of responses $\\mathbf{y}$ is not in $\\text{Col}(\\mathbf{X})$.\n\n-   **Goal:** Find another vector $\\mathbf{z} = \\mathbf{Xb}$ that is in $\\text{Col}(\\mathbf{X})$ and is as close as possible to $\\mathbf{y}$.\n\n    -   $\\mathbf{z}$ is called a **projection** of $\\mathbf{y}$ onto $\\text{Col}(\\mathbf{X})$ .\n:::\n\n## Geometry of least squares regression\n\n::: incremental\n-   For any $\\mathbf{z} = \\mathbf{Xb}$ in $\\text{Col}(\\mathbf{X})$, the vector $\\mathbf{e} = \\mathbf{y} - \\mathbf{Xb}$ is the difference between $\\mathbf{y}$ and $\\mathbf{Xb}$.\n\n    -   In other words, we want to minimize $||\\mathbf{e}||^2 = ||\\mathbf{y} - \\mathbf{Xb}||^2$\n\n-   This is minimized for the $\\mathbf{b}$ ( we'll call it $\\hat{\\boldsymbol{\\beta}}$ ) that makes $\\mathbf{e}$ orthogonal to $\\text{Col}(\\mathbf{X})$\n\n-   **Recall**: If $\\mathbf{e}$ is orthogonal to $\\text{Col}(\\mathbf{X})$, then the inner product of any vector in $\\text{Col}(\\mathbf{X})$ and $\\mathbf{e}$ is 0 $\\Rightarrow \\mathbf{X}^T\\mathbf{e} = \\mathbf{0}$\n:::\n\n## Geometry of least squares regression\n\n-   Therefore, we have\n\n$$\n\\mathbf{X}^T(\\mathbf{y} - \\mathbf{Xb}) = \\mathbf{0}\n$$\n\nLet's solve for $\\mathbf{b}$ to get the least squares estimate.\n\n## Recap\n\n-   Compared models using Adjusted $R^2$\n\n-   Introduced the ANOVA table\n\n-   Used a geometric interpretation to find the least squares estimates\n\n## Next class\n\n-   Inference for regression\n\n-   See Sep 19 prepare\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}