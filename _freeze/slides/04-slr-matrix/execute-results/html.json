{
  "hash": "d57d8405470bada40532decfe02de72c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"SLR: Matrix representation\"\nauthor: \"Prof. Maria Tackett\"\ndate: \"2024-09-05\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— STA 221 - Fall 2024](https://sta221-fa24.netlify.app)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n\n\n\n\n## Topics\n\n-   Matrix representation for simple linear regression\n    -   Model form\n    -   Least square estimate\n    -   Predicted (fitted) values\n    -   Residuals\n-   Matrix representation in R\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n# Matrix representation of simple linear regression\n\n## SLR: Statistical model (population)\n\nWhen we have a quantitative response, $Y$, and a single quantitative predictor, $X$, we can use a **simple linear regression** model to describe the relationship between $Y$ and $X$. $$\\large{Y = \\mathbf{\\beta_0 + \\beta_1 X} + \\epsilon}, \\hspace{8mm} \\epsilon \\sim N(0, \\sigma_{\\epsilon}^2)$$\n\n<br>\n\n-   $\\beta_1$: Population (true) slope of the relationship between $X$ and $Y$\n-   $\\beta_0$: Population (true) intercept of the relationship between $X$ and $Y$\n-   $\\epsilon$: Error\n\n## SLR in matrix form\n\nSuppose we have $n$ observations.\n\n$$\n\\underbrace{\n\\begin{bmatrix}\ny_1 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix} }_\n{\\mathbf{y}} \\hspace{3mm}\n= \n\\hspace{3mm}\n\\underbrace{\n\\begin{bmatrix}\n1 &x_1 \\\\\n\\vdots &  \\vdots \\\\\n1 &  x_n\n\\end{bmatrix}\n}_{\\mathbf{X}}\n\\hspace{2mm}\n\\underbrace{\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1\n\\end{bmatrix}\n}_{\\boldsymbol{\\beta}}\n\\hspace{3mm}\n+\n\\hspace{3mm}\n\\underbrace{\n\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\vdots\\\\\n\\epsilon_n\n\\end{bmatrix}\n}_\\boldsymbol{\\epsilon}\n$$\n\n<br>\n\n::: question\nWhat are the dimensions of $\\mathbf{y}$, $\\mathbf{X}$, $\\boldsymbol{\\beta}$, and $\\boldsymbol{\\epsilon}$?\n:::\n\n## Sum of squared residuals\n\nWe use the sum of squared residuals (also called \"sum of squared error\") to find the least squares line:\n\n$$\nSSR = \\sum_{i=1}^ne_i^2 = \\mathbf{e}^T\\mathbf{e} = (\\mathbf{y} - \\hat{\\mathbf{y}})^T(\\mathbf{y} - \\hat{\\mathbf{y}})\n$$\n\n<br>\n\n::: question\n-   What is the dimension of SSR?\n\n-   What is $\\hat{\\mathbf{y}}$ in terms of $\\mathbf{y}$, $\\mathbf{X}$, and/or $\\boldsymbol{\\beta}$ ?\n:::\n\n## Minimize sum of squared residuals\n\nWe want to find values of $\\boldsymbol{\\beta} = \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix}$ that minimize the sum of squared residuals $$\n\\begin{aligned}\n\\mathbf{e}^T\\mathbf{e} &= (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\[10pt]\n\\end{aligned}\n$$\n\n## Minimize sum of squared residuals\n\nWe want to find values of $\\boldsymbol{\\beta} = \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix}$ that minimize the sum of squared residuals $$\n\\begin{aligned}\n\\mathbf{e}^T\\mathbf{e} &= (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\[10pt]\n&= (\\mathbf{y}^T - \\boldsymbol{\\beta}^T\\mathbf{X}^T)(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\\\[10pt]\n\\end{aligned}\n$$\n\n## Minimize sum of squared residuals\n\nWe want to find values of $\\boldsymbol{\\beta} = \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix}$ that minimize the sum of squared residuals $$\n\\begin{aligned}\n\\mathbf{e}^T\\mathbf{e} &= (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\[10pt]\n&= (\\mathbf{y}^T - \\boldsymbol{\\beta}^T\\mathbf{X}^T)(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\\\[10pt]\n&=\\mathbf{y}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\\\\[10pt]\n\\end{aligned}\n$$\n\n## Minimize sum of squared residuals\n\nWe want to find values of $\\boldsymbol{\\beta} = \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix}$ that minimize the sum of squared residuals $$\n\\begin{aligned}\n\\mathbf{e}^T\\mathbf{e} &= (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\[10pt]\n&= (\\mathbf{y}^T - \\boldsymbol{\\beta}^T\\mathbf{X}^T)(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\\\[10pt]\n&=\\mathbf{y}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\\\\[10pt]\n&=\\mathbf{y}^T\\mathbf{y} - 2\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\n\\end{aligned}\n$$\n\n## Least squares estimators\n\n$$\nSSR = \\mathbf{e}^T\\mathbf{e} =\\mathbf{y}^T\\mathbf{y} - 2\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\n$$\n\n<br>\n\n. . .\n\nThe least squares estimators must satisfy\n\n$$\n\\nabla_{\\boldsymbol{\\beta}} SSR = -2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = 0\n$$\n\n<br>\n\n. . .\n\n$$\n\\color{#993399}{\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}}\n$$\n\n## Did we find a minimum?\n\n$$\n\\nabla^2_{\\beta} SSR \\propto  2\\mathbf{X}^T\\mathbf{X} = 0\n$$\n\n<br>\n\n-   $\\mathbf{X}$ is full rank $\\Rightarrow$ $\\mathbf{X}^T\\mathbf{X}$ is positive definite\n\n-   Therefore we have found the minimizing point\n\n# Matrix representation in R\n\n## Obtain $\\mathbf{y}$ vector\n\nLet's go back to the Duke Forest data. We want to use the matrix representation to fit a model of the form:\n\n$$\nprice = \\beta_0 + \\beta_1 ~ area + \\epsilon, \\hspace{5mm} \\epsilon \\sim N(0, \\sigma^2_\\epsilon)\n$$\n\n. . .\n\nGet $\\mathbf{y}$, the vector of responses\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ny <- duke_forest$price\n```\n:::\n\n\n\n<br>\n\n. . .\n\nLet's look at the first 10 observations of $y$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ny[1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 1520000 1030000  420000  680000  428500  456000 1270000  557450  697500\n[10]  650000\n```\n\n\n:::\n:::\n\n\n\n## Obtain $\\mathbf{X}$ matrix\n\nUse the `model.matrix()` function to get $\\mathbf{X}$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nX <- model.matrix(price ~ area, data = duke_forest)\n```\n:::\n\n\n\n<br>\n\n. . .\n\nLet's look at the first 10 rows of $\\mathbf{X}$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nX[1:10,]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   (Intercept) area\n1            1 6040\n2            1 4475\n3            1 1745\n4            1 2091\n5            1 1772\n6            1 1950\n7            1 3909\n8            1 2841\n9            1 3924\n10           1 2173\n```\n\n\n:::\n:::\n\n\n\n## Calculate $\\hat{\\boldsymbol{\\beta}}$\n\nMatrix functions in R. Let $\\mathbf{A}$ and $\\mathbf{B}$ be matrices\n\n-   `t(A)`: transpose $\\mathbf{A}$\n-   `solve(A)`: inverse of $\\mathbf{A}$\n-   `A %*% B`: multiply $\\mathbf{A}$ and $\\mathbf{B}$\n\n. . .\n\nNow let's calculate $\\hat{\\boldsymbol{\\beta}}$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbeta_hat <- solve(t(X)%*%X)%*%t(X)%*%y\nbeta_hat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   [,1]\n(Intercept) 116652.3251\narea           159.4833\n```\n\n\n:::\n:::\n\n\n\n## Compare to result from `lm`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nduke_forest_model <- lm(price ~ area, data = duke_forest)\ntidy(duke_forest_model) |> kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|term        |   estimate| std.error| statistic| p.value|\n|:-----------|----------:|---------:|---------:|-------:|\n|(Intercept) | 116652.325| 53302.463|     2.188|   0.031|\n|area        |    159.483|    18.171|     8.777|   0.000|\n\n\n:::\n:::\n\n\n\n<br>\n\n. . .\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbeta_hat \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   [,1]\n(Intercept) 116652.3251\narea           159.4833\n```\n\n\n:::\n:::\n\n\n\n# Predicted values and residuals\n\n## Predicted (fitted) values\n\nNow that we have $\\hat{\\boldsymbol{\\beta}}$, let's predict values of $\\mathbf{y}$ using the model\n\n$$\n\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\underbrace{\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T}_{\\mathbf{H}}\\mathbf{y} = \\mathbf{H}\\mathbf{y}\n$$\n\n. . .\n\n**Hat matrix**: $\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T$\n\n-   $\\mathbf{H}$ is an $n\\times n$ matrix\n-   Maps vector of observed values $\\mathbf{y}$ to a vector of fitted values $\\hat{\\mathbf{y}}$\n\n## Residuals\n\nRecall that the residuals are the difference between the observed and predicted values\n\n$$\n\\begin{aligned}\n\\mathbf{e} &= \\mathbf{y} - \\hat{\\mathbf{y}}\\\\[10pt]\n& = \\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt]\n& = \\mathbf{y} - \\mathbf{H}\\mathbf{y} \\\\[10pt]\n& = (\\mathbf{I} - \\mathbf{H})\\mathbf{y}\n\\end{aligned}\n$$\n\n. . .\n\n$$\n\\color{#993399}{\\mathbf{e} = (\\mathbf{I} - \\mathbf{H})\\mathbf{y}}\n$$\n\n## Recap\n\n-   Introduced matrix representation for simple linear regression\n    -   Model from\n    -   Least square estimate\n    -   Predicted (fitted) values\n    -   Residuals\n-   Used R for matrix calculations\n\n## Next class\n\n-   Multiple linear regression\n\n-   See [Sep 10 prepare](../prepare/prepare-sep10.html)\n",
    "supporting": [
      "04-slr-matrix_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}