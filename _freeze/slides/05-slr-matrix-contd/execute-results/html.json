{
  "hash": "ad196002c5f2b35023f98cf17e20c185",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"SLR: Matrix representation\"\nsubtitle: \"cont'd\"\nauthor: \"Prof. Maria Tackett\"\ndate: \"2024-09-10\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— STA 221 - Fall 2024](https://sta221-fa24.netlify.app)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n\n\n\n\n## Announcements\n\n-   Lab 01 due on **Thursday, September 12 at 11:59pm**\n\n    -   Push work to GitHub repo\n\n    -   Submit final PDF on Gradescope + mark pages for each question\n\n-   HW 01 will be assigned on Thursday\n\n## Topics\n\n-   Matrix representation for simple linear regression\n    -   Model form\n    -   Least square estimate\n    -   Predicted (fitted) values\n    -   Residuals\n-   Matrix representation in R\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n# Matrix representation of simple linear regression\n\n## SLR in matrix form {.midi}\n\nSuppose we have $n$ observations, a quantitative response variable, and a single predictor$$\n\\underbrace{\n\\begin{bmatrix}\ny_1 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix} }_\n{\\mathbf{y}} \\hspace{3mm}\n= \n\\hspace{3mm}\n\\underbrace{\n\\begin{bmatrix}\n1 &x_1 \\\\\n\\vdots &  \\vdots \\\\\n1 &  x_n\n\\end{bmatrix}\n}_{\\mathbf{X}}\n\\hspace{2mm}\n\\underbrace{\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1\n\\end{bmatrix}\n}_{\\boldsymbol{\\beta}}\n\\hspace{3mm}\n+\n\\hspace{3mm}\n\\underbrace{\n\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\vdots\\\\\n\\epsilon_n\n\\end{bmatrix}\n}_\\boldsymbol{\\epsilon}\n$$\n\n<br>\n\n-   $\\mathbf{y}$: $n\\times 1$ vector of responses\n-   $\\mathbf{X}$: $n \\times 2$ design matrix <!--# another way to describe this without using design matrix?-->\n-   $\\boldsymbol{\\beta}$: $2 \\times 1$ vector of coefficients\n-   $\\boldsymbol{\\epsilon}$: $n \\times 1$ vector of error terms\n\n## Minimize sum of squared residuals\n\n**Goal**: Find values of $\\boldsymbol{\\beta} = \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix}$ that minimize the sum of squared residuals $$\n\\begin{aligned}\nSSR = \\sum_{i=1}^n e_i^2 = \\mathbf{e}^T\\mathbf{e} &= (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\[10pt]\n\\end{aligned}\n$$\n\nMinimize sum of squared residuals $$\n\\begin{aligned}\nSSR = \\mathbf{e}^T\\mathbf{e} &= (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\[10pt]\n&= (\\mathbf{y}^T - \\boldsymbol{\\beta}^T\\mathbf{X}^T)(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\\\[10pt]\n\\end{aligned}\n$$\n\n## Minimize sum of squared residuals\n\n$$\n\\begin{aligned}\n\\mathbf{e}^T\\mathbf{e} &= (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\[10pt]\n&= (\\mathbf{y}^T - \\boldsymbol{\\beta}^T\\mathbf{X}^T)(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\\\[10pt]\n&=\\mathbf{y}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\\\\[10pt]\n\\end{aligned}\n$$\n\n. . .\n\n$\\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta}$ and $\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y}$ are scalars. Therefore, $\\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta} = (\\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta})^T = \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y}$\n\n## Minimize sum of squared residuals\n\n$$\n\\begin{aligned}\n\\mathbf{e}^T\\mathbf{e} &= (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) \\\\[10pt]\n&= (\\mathbf{y}^T - \\boldsymbol{\\beta}^T\\mathbf{X}^T)(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\\\[10pt]\n&=\\mathbf{y}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\\\\[10pt]\n&=\\mathbf{y}^T\\mathbf{y} - 2\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}\n\\end{aligned}\n$$\n\n## Minimize sum of squared residuals\n\nThe estimate of $\\boldsymbol{\\beta} = \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix}$ that minimize SSR is the one such that\n\n$$\n\\nabla_{\\boldsymbol{\\beta}} SSR = \\nabla_{\\boldsymbol{\\beta}}( \\mathbf{y}^T\\mathbf{y} - 2\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}) = 0\n$$\n\n## Side note: Vector operations {.midi}\n\nLet $\\mathbf{x} = \\begin{bmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\x_k\\end{bmatrix}$be a $k \\times 1$ vector and $f(\\mathbf{x})$ be a function of $\\mathbf{x}$.\n\nThen $\\nabla_\\mathbf{x}f$, the **gradient** of $f$ with respect to $\\mathbf{x}$ is\n\n$$\n\\nabla_\\mathbf{x}f = \\begin{bmatrix}\\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_k}\\end{bmatrix}\n$$\n\n## Side note: Vector operations\n\n::: {.callout-note icon=\"false\"}\n## Proposition 1\n\nLet $\\mathbf{x}$ be a $k \\times 1$ vector and $\\mathbf{z}$ be a $k \\times 1$ vector, such that $\\mathbf{z}$ is not a function of $\\mathbf{x}$ .\n\nThe gradient of $\\mathbf{x}^T\\mathbf{z}$ with respect to $\\mathbf{x}$ is\n\n$$\n\\nabla_\\mathbf{x} \\hspace{1mm} \\mathbf{x}^T\\mathbf{z} = \\mathbf{z}\n$$\n:::\n\n## Side note: Proposition 1\n\n$$\n\\begin{aligned}\n\\mathbf{x}^T\\mathbf{z} &= \\begin{bmatrix}x_1 & x_2 & \\dots &x_k\\end{bmatrix} \n \\begin{bmatrix}z_1 \\\\ z_2 \\\\ \\vdots \\\\z_k\\end{bmatrix} \\\\[10pt]\n &= x_1z_1 + x_2z_2 + \\dots + x_kz_k\n\\end{aligned}\n$$\n\n## Side note: Proposition 1 {.midi}\n\n$$\n\\nabla_\\mathbf{x}\\hspace{1mm}\\mathbf{x}^T\\mathbf{z} = \\begin{bmatrix}\\frac{\\partial \\mathbf{x}^T\\mathbf{z}}{\\partial x_1} \\\\ \\frac{\\partial \\mathbf{x}^T\\mathbf{z}}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial \\mathbf{x}^T\\mathbf{z}}{\\partial x_k}\\end{bmatrix}  \n= \\begin{bmatrix}\\frac{\\partial}{\\partial x_1} (x_1z_1 + x_2z_2 + \\dots + x_kz_k) \\\\ \\frac{\\partial}{\\partial x_2} (x_1z_1 + x_2z_2 + \\dots + x_kz_k)\\\\ \\vdots \\\\ \\frac{\\partial}{\\partial x_k} (x_1z_1 + x_2z_2 + \\dots + x_kz_k)\\end{bmatrix}\n = \\begin{bmatrix} z_1 \\\\ z_2 \\\\ \\vdots \\\\ z_k\\end{bmatrix} = \\mathbf{z}\n$$\n\n## Side note: Vector + matrix operations\n\n::: {.callout-note icon=\"false\"}\n## Proposition 2\n\nLet $\\mathbf{x}$ be a $k \\times 1$ vector and $\\mathbf{A}$ be a $k \\times k$ matrix, such that $\\mathbf{A}$ is not a function of $\\mathbf{x}$ .\n\nThen the gradient of $\\mathbf{x}^T\\mathbf{A}\\mathbf{x}$ with respect to $\\mathbf{x}$ is\n\n$$\n\\nabla_\\mathbf{x} \\hspace{1mm} \\mathbf{x}^T\\mathbf{A}\\mathbf{x} = (\\mathbf{A}\\mathbf{x} + \\mathbf{A}^T \\mathbf{x}) = (\\mathbf{A} + \\mathbf{A}^T)\\mathbf{x}\n$$\n\nIf $\\mathbf{A}$ is symmetric, then\n\n$$\n(\\mathbf{A} + \\mathbf{A}^T)\\mathbf{x} = 2\\mathbf{A}\\mathbf{x}\n$$\n:::\n\n## Side note: Proposition 2\n\n## Find the least squares estimators\n\n$$\n\\begin{aligned}\n\\nabla_{\\boldsymbol{\\beta}} SSR &= \\nabla_{\\boldsymbol{\\beta}}( \\mathbf{y}^T\\mathbf{y} - 2\\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}) = 0 \\\\[10pt]\n& = \\nabla_\\boldsymbol{\\beta} \\hspace{1mm} \\mathbf{y}^T\\mathbf{y} - 2\\nabla_\\boldsymbol{\\beta} \\hspace{1mm} \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\nabla_\\boldsymbol{\\beta} \\hspace{1mm} \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} \\\\[10pt]\n&= 0 - \\underbrace{2\\mathbf{X}^T\\mathbf{y}}_{\\text{Prop }1} + \\underbrace{2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta}}_{\\text{Prop }2}\n\\end{aligned}\n$$\n\n<!--# finish adding the steps here-->\n\n. . .\n\n$$\n\\color{#993399}{\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}}\n$$\n\n## Did we find a minimum?\n\n$$\n\\nabla^2_{\\beta} SSR \\propto  2\\mathbf{X}^T\\mathbf{X} = 0\n$$\n\n<br>\n\n-   $\\mathbf{X}$ is full rank $\\Rightarrow$ $\\mathbf{X}^T\\mathbf{X}$ is positive definite\n\n-   Therefore we have found the minimizing point\n\n# Matrix representation in R\n\n## Obtain $\\mathbf{y}$ vector\n\nLet's go back to the Duke Forest data. We want to use the matrix representation to fit a model of the form:\n\n$$\nprice = \\beta_0 + \\beta_1 ~ area + \\epsilon, \\hspace{5mm} \\epsilon \\sim N(0, \\sigma^2_\\epsilon)\n$$\n\n. . .\n\nGet $\\mathbf{y}$, the vector of responses\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ny <- duke_forest$price\n```\n:::\n\n\n\n<br>\n\n. . .\n\nLet's look at the first 10 observations of $y$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ny[1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 1520000 1030000  420000  680000  428500  456000 1270000  557450  697500\n[10]  650000\n```\n\n\n:::\n:::\n\n\n\n## Obtain $\\mathbf{X}$ matrix\n\nUse the `model.matrix()` function to get $\\mathbf{X}$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nX <- model.matrix(price ~ area, data = duke_forest)\n```\n:::\n\n\n\n<br>\n\n. . .\n\nLet's look at the first 10 rows of $\\mathbf{X}$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nX[1:10,]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   (Intercept) area\n1            1 6040\n2            1 4475\n3            1 1745\n4            1 2091\n5            1 1772\n6            1 1950\n7            1 3909\n8            1 2841\n9            1 3924\n10           1 2173\n```\n\n\n:::\n:::\n\n\n\n## Calculate $\\hat{\\boldsymbol{\\beta}}$\n\nMatrix functions in R. Let $\\mathbf{A}$ and $\\mathbf{B}$ be matrices\n\n-   `t(A)`: transpose $\\mathbf{A}$\n-   `solve(A)`: inverse of $\\mathbf{A}$\n-   `A %*% B`: multiply $\\mathbf{A}$ and $\\mathbf{B}$\n\n. . .\n\nNow let's calculate $\\hat{\\boldsymbol{\\beta}}$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbeta_hat <- solve(t(X)%*%X)%*%t(X)%*%y\nbeta_hat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   [,1]\n(Intercept) 116652.3251\narea           159.4833\n```\n\n\n:::\n:::\n\n\n\n## Compare to result from `lm`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nduke_forest_model <- lm(price ~ area, data = duke_forest)\ntidy(duke_forest_model) |> kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|term        |   estimate| std.error| statistic| p.value|\n|:-----------|----------:|---------:|---------:|-------:|\n|(Intercept) | 116652.325| 53302.463|     2.188|   0.031|\n|area        |    159.483|    18.171|     8.777|   0.000|\n\n\n:::\n:::\n\n\n\n<br>\n\n. . .\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbeta_hat \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   [,1]\n(Intercept) 116652.3251\narea           159.4833\n```\n\n\n:::\n:::\n\n\n\n# Predicted values and residuals\n\n## Predicted (fitted) values\n\nNow that we have $\\hat{\\boldsymbol{\\beta}}$, let's predict values of $\\mathbf{y}$ using the model\n\n$$\n\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\underbrace{\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T}_{\\mathbf{H}}\\mathbf{y} = \\mathbf{H}\\mathbf{y}\n$$\n\n. . .\n\n**Hat matrix**: $\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T$\n\n-   $\\mathbf{H}$ is an $n\\times n$ matrix\n-   Maps vector of observed values $\\mathbf{y}$ to a vector of fitted values $\\hat{\\mathbf{y}}$\n\n## Residuals\n\nRecall that the residuals are the difference between the observed and predicted values\n\n$$\n\\begin{aligned}\n\\mathbf{e} &= \\mathbf{y} - \\hat{\\mathbf{y}}\\\\[10pt]\n& = \\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt]\n& = \\mathbf{y} - \\mathbf{H}\\mathbf{y} \\\\[10pt]\n& = (\\mathbf{I} - \\mathbf{H})\\mathbf{y}\n\\end{aligned}\n$$\n\n. . .\n\n$$\n\\color{#993399}{\\mathbf{e} = (\\mathbf{I} - \\mathbf{H})\\mathbf{y}}\n$$\n\n## Recap\n\n-   Introduced matrix representation for simple linear regression\n    -   Model from\n    -   Least square estimate\n    -   Predicted (fitted) values\n    -   Residuals\n-   Used R for matrix calculations\n\n## Next class\n\n-   Multiple linear regression\n\n-   See [Sep 10 prepare](../prepare/prepare-sep10.html)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}