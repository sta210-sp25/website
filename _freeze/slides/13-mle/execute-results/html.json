{
  "hash": "22a2319872600267998cbe76aacb169e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Maximum likelihood estimation\"\nauthor: \"Prof. Maria Tackett\"\ndate: \"2024-10-10\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[üîó STA 221 - Fall 2024](https://sta221-fa24.netlify.app)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    include-before: [ '<script type=\"text/x-mathjax-config\">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nfilters:\n  - parse-latex\nbibliography: references.bib\n---\n\n\n\n## Announcements\n\n-   Office hours:\n\n    -   This week: Thursday - Friday\n\n    -   Next week: Wednesday - Friday\n\n-   No class next Monday or Tuesday\n\n<br>\n\n<center> üçÅ Have a good Fall Break! üçÅ </center>\n\n## Topics\n\n-   Likelihood\n\n-   Maximum likelihood estimation\n\n-   MLE for linear regression\n\n-   Properties of maximum likelihood estimator\n\n## Motivation  {.midi}\n\n-   We can find the estimators of $\\boldsymbol{\\beta}$ and $\\sigma^2_{\\epsilon}$ for the model\n\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}, \\hspace{10mm} \\boldsymbol{\\epsilon} \\sim N(0, \\sigma^2_\\epsilon\\mathbf{I})\n$$using least-squares estimation\n\n-   We have also shown some nice properties of the least-squares estimator $\\hat{\\boldsymbol{\\beta}}$ , given $E(\\boldsymbol{\\epsilon}) = \\mathbf{0}$ and $Var(\\boldsymbol{\\epsilon}) = \\sigma^2_{\\epsilon}\\mathbf{I}$\n\n-   Today we will introduce another way to find these estimators - **maximum likelihood estimation.** We will see...\n\n    -   the maximum likelihood estimators have nice properties\n\n    -   the least-squares estimator is equal to the maximum likelihood estimator when certain assumptions hold\n\n# Maximum likelihood estimation\n\n## Example: Shooting free throws\n\nSuppose a basketball player shoots a single free throw, such that the probability of making a basket is $p$\n\n-   What is the probability distribution for this random phenomenon?\n\n-   Suppose the probability is $p = 0.5$? What is the probability the player makes a single shot, given this value of the parameter $p$?\n\n-   Suppose the probability is $p = 0.8$? What is the probability the player makes a single shot, given this value of the parameter $p$?\n\n## Shooting three free throws\n\nSuppose the player shoots three free throws. They are all independent and the player has the same probability $p$ of making each shot.\n\nLet $B$ represent a made basket, and $M$ represent a missed basket. The player shoots three free throws with the outcome $BBM$.\n\n-   Suppose the probability is $p = 0.5$? What is the probability of observing the data $BBM$, given this value of the parameter $p$?\n\n-   Suppose the probability is $p = 0.3$? What is the probability of observing the data $BBM$, given this value of the parameter $p$ ?\n\n## Shooting three free throws\n\nSuppose the player shoots three free throws. They are all independent and the player has the same probability $p$ of making each shot.\n\nThe player shoots three free throws with the outcome $BBM$.\n\n-   How would you describe in words the probabilities we previously calculated?\n\n-   **New question:** What parameter value of $p$ do you think maximizes the\n    probability of observing this data?\n\n-   We will use a **likelihood** function to answer this question.\n\n## Likelihood\n\nA **likelihood** is a function that tells us how likely we are to observe our data for a given parameter value (or values). <!--# Find a better definition - Casella Berger maybe?-->\n\n. . .\n\nNote that this is **not** the same as the probability function. In other words,\n\n. . .\n\n**Probability function**: Fixed parameter value(s) + input possible outcomes $\\Rightarrow$ probability of seeing the different outcomes given the parameter value(s)\n\n**Likelihood function**: Fixed data + input possible parameter values $\\Rightarrow$ probability of seeing the fixed data for each parameter value\n\n## Likelihood: shooting three free throws\n\nThe likelihood function for the probability of heads $p$ given we observed $BBM$ when shooting three independent free throws is $$\nL(p|BBM) = p \\times p \\times (1 - p)\n$$\n\nThus, if the likelihood for $p = 0.8$ is\n\n$$\nL(p = 0.8|BBM) = 0.8 \\times 0.8 \\times (1 - 0.8) = 0.128\n$$\n\n## Likelihood: shooting three free throws\n\n-   What is the general formula for the likelihood function for $p$ given the observed data $BBM$?\n\n-   Why do we need to assume independence?\n\n-   Why does having identically distributed data simplify things?\n\n## Likelihood: shooting three free throws {.midi}\n\nThe likelihood function for $p$ given the data $BBM$ is\n\n$$\nL(p|BBM) = p \\times p \\times (1 - p) = p^2 \\times (1 - p)\n$$\n\n-   We want of the value of $p$ that maximizes this likelihood function, i.e., the value of $p$ that is most likely given the observed data.\n\n-   The process of finding this value is **maximum likelihood estimation**.\n\n-   There are three primary ways to find the maximum likelihood estimator\n\n    -   Approximate using a graph\n\n    -   Using calculus\n\n    -   Numerical approximation\n\n<!--# Add a poll - what is your guess for the MLE?-->\n\n## Finding the MLE using graphs\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](13-mle_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n\n::: question\nWhat do you think is the approximate value of the MLE of $p$ given the data?\n:::\n\n## Finding the MLE using calculus\n\n-   Find the MLE using the first derivative of the likelihood function.\n\n\n\n```{=html}\n<!-- -->\n```\n\n\n-   This can be tricky because of the Product Rule, so we can maximize the **log(Likelihood)** instead. The same value maximizes the likelihood and log(Likelihood).\n\n::: question\nUse calculus to find the MLE of $p$ given the data $BBM$.\n:::\n\n## Shooting $n$ free throws\n\nSuppose the player shoots $n$ free throw. They are all independent and the player has the same probability $p$ of making each shot.\n\nSuppose the player makes $k$ baskets out of the $n$ free throws. This is the observe data.\n\n-   What is the formula for the probability distribution to describe this random phenomenon?\n\n-   What is the formula for the likelihood function for $p$ given the observed data?\n\n-   For what value of $p$ do we maximize the likelihood given the observed data? Use calculus to find the response.\n\n## Why maximum likelihood estimation?  {.midi}\n\n-   *\"Maximum likelihood estimation is, by far, the most popular technique for deriving estimators.\"* [@casella2024statistical, pp. 315]\n\n-   MLEs have nice statistical properties. They are\n\n    -   Consistent\n\n    -   Efficient - Have the smallest MSE among all consistent estimators\n\n    -   Asymptotically normal\n\n. . .\n\n::: callout-note\nIf the normality assumption holds, the least squares estimator is the maximum likelihood estimator for $\\beta$. Therefore, it has all these properties of the MLE.\n:::\n\n# MLE in linear regression\n\n<!--# intro slide of this is our model we've seen the whole time-->\n\n<!--# slide looking at the likelihood and log-likelihood for simple linear regression model-->\n\n<!--# maybe show the work for beta 0-->\n\n<!--# show the result for beta1 and sigma. You will explore this more in lab after fall break-->\n\n## Recap\n\n## References\n",
    "supporting": [
      "13-mle_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}