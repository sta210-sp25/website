{
  "hash": "92ace5755ef30fdf2259051c0e398175",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Model comparison + cross validation\"\nauthor: \"Prof. Maria Tackett\"\ndate: \"2023-10-18\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[🔗 STA 210 - Fall 2023 -  Schedule](https://sta210-fa23.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n\n\n\n\n## Announcements\n\n-   Exam 02 - April 17 (same format as Exam 01)\n\n    -   Lecture videos available\n\n-   Next project milestone: Draft and peer review in April 21 lab\n\n-   Statistics experience due April 15\n\n## Topics\n\n::: nonincremental\n-   Cross validation\n:::\n\n## Computational setup\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))\n```\n:::\n\n\n\n# Application exercise\n\n::: appex\n📋 <https://sta210-sp25.netlify.app/ae/ae-11-multinomial.html>\n:::\n\n# Cross validation\n\n## Spending our data\n\n-   We have already established that the idea of data spending where the test set was recommended for obtaining an unbiased estimate of performance.\n-   However, we usually need to understand the effectiveness of the model [*before*]{.underline} *using the test set*.\n-   Typically we can't decide on *which* final model to take to the test set without making model assessments.\n-   **Remedy:** Resampling to make model assessments on training data in a way that can generalize to new data.\n\n## Resampling for model assessment\n\n**Resampling is only conducted on the** <u>**training**</u> **set**. The test set is not involved. For each iteration of resampling, the data are partitioned into two subsamples:\n\n-   The model is fit with the **analysis set**. Model fit statistics such as $R^2_{Adj}$, AIC, and BIC are calculated based on this fit.\n-   The model is evaluated with the **assessment set**.\n\n## Resampling for model assessment\n\n![](images/23/resampling.svg){fig-align=\"center\"}\n\n<br>\n\nImage source: Kuhn and Silge. [Tidy modeling with R](https://www.tmwr.org/).\n\n## Analysis and assessment sets\n\n-   Analysis set is analogous to training set.\n-   Assessment set is analogous to test set.\n-   The terms *analysis* and *assessment* avoids confusion with initial split of the data.\n-   These data sets are mutually exclusive.\n\n## Cross validation\n\nMore specifically, **v-fold cross validation** -- commonly used resampling technique:\n\n-   Randomly split your **training** **data** into ***v*** partitions\n-   Use ***v-1*** partitions for analysis, and the remaining 1 partition for analysis (model fit + model fit statistics)\n-   Repeat ***v*** times, updating which partition is used for assessment each time\n\n. . .\n\nLet's give an example where `v = 3`...\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n## To get started...\n\n**Split data into training and test sets**\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(345)\n\nsesame_split <- initial_split(sesame)\nsesame_train <- training(sesame_split)\nsesame_test <- testing(sesame_split)\n```\n:::\n\n\n\n## To get started... {.midi}\n\n**Specify model**\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsesame_spec <- multinom_reg()\n```\n:::\n\n\n\n<br> . . .\n\n**Note: Use `linear_reg()` or `logistic_reg()` for linear or logistic models, respectively.**\n\n**Create workflow**\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsesame_wflow1 <- workflow() |>\n  add_model(sesame_spec) |>\n  add_formula(viewcat ~ ageCent + viewenc)\n\nsesame_wflow1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: multinom_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nviewcat ~ ageCent + viewenc\n\n── Model ───────────────────────────────────────────────────────────────────────\nMultinomial Regression Model Specification (classification)\n\nComputational engine: nnet \n```\n\n\n:::\n:::\n\n\n\n## Cross validation, step 1\n\nRandomly split your **training** **data** into 3 partitions:\n\n<br>\n\n![](images/23/three-CV.svg){fig-align=\"center\"}\n\n## Tips: Split training data\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfolds <- vfold_cv(sesame_train, v = 3)\nfolds\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#  3-fold cross-validation \n# A tibble: 3 × 2\n  splits           id   \n  <list>           <chr>\n1 <split [120/60]> Fold1\n2 <split [120/60]> Fold2\n3 <split [120/60]> Fold3\n```\n\n\n:::\n:::\n\n\n\n## Cross validation, steps 2 and 3\n\n::: nonincremental\n-   Use *v-1* partitions for analysis, and the remaining 1 partition for assessment\n-   Repeat *v* times, updating which partition is used for assessment each time\n:::\n\n![](images/23/three-CV-iter.svg){fig-align=\"center\"}\n\n## Sesame Street: Fit resamples {.midi}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsesame_fit_rs1 <- sesame_wflow1 |>\n  fit_resamples(resamples = folds,\n               metrics = metric_set(accuracy, roc_auc))\n\nsesame_fit_rs1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Resampling results\n# 3-fold cross-validation \n# A tibble: 3 × 4\n  splits           id    .metrics         .notes          \n  <list>           <chr> <list>           <list>          \n1 <split [120/60]> Fold1 <tibble [2 × 4]> <tibble [0 × 3]>\n2 <split [120/60]> Fold2 <tibble [2 × 4]> <tibble [0 × 3]>\n3 <split [120/60]> Fold3 <tibble [2 × 4]> <tibble [0 × 3]>\n```\n\n\n:::\n:::\n\n\n\n## Cross validation, now what?\n\n-   We've fit a bunch of models\n-   Now it's time to use them to collect metrics (e.g., AUC, ) on each model and use them to evaluate model fit and how it varies across folds\n\n## Collect metrics from CV\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Produces summary across all CV\ncollect_metrics(sesame_fit_rs1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy multiclass 0.356     3  0.0547 Preprocessor1_Model1\n2 roc_auc  hand_till  0.597     3  0.0223 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\n\n<br>\n\nNote: These are calculated using the *assessment* data\n\n## Deeper look into results\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv_metrics1 <- collect_metrics(sesame_fit_rs1, summarize = FALSE) \n\ncv_metrics1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 5\n  id    .metric  .estimator .estimate .config             \n  <chr> <chr>    <chr>          <dbl> <chr>               \n1 Fold1 accuracy multiclass     0.383 Preprocessor1_Model1\n2 Fold1 roc_auc  hand_till      0.598 Preprocessor1_Model1\n3 Fold2 accuracy multiclass     0.25  Preprocessor1_Model1\n4 Fold2 roc_auc  hand_till      0.558 Preprocessor1_Model1\n5 Fold3 accuracy multiclass     0.433 Preprocessor1_Model1\n6 Fold3 roc_auc  hand_till      0.635 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\n\n## Better presentation of results\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv_metrics1 |>\n  mutate(.estimate = round(.estimate, 3)) |>\n  pivot_wider(id_cols = id, names_from = .metric, values_from = .estimate) |>\n  kable(col.names = c(\"Fold\", \"Accuracy\", \"AUC\"))\n```\n\n::: {.cell-output-display}\n\n\n|Fold  | Accuracy|   AUC|\n|:-----|--------:|-----:|\n|Fold1 |    0.383| 0.598|\n|Fold2 |    0.250| 0.558|\n|Fold3 |    0.433| 0.635|\n\n\n:::\n:::\n\n\n\n## Cross validation in practice\n\n::: incremental\n-   To illustrate how CV works, we used `v = 3`:\n\n    ::: nonincremental\n    -   Analysis sets are 2/3 of the training set\n    -   Each assessment set is a distinct 1/3\n    -   The final resampling estimate of performance averages each of the 3 replicates\n    :::\n\n-   This was useful for illustrative purposes, but `v` is often 5 or 10; we generally prefer 10-fold cross-validation as a default\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}