{
  "hash": "cefa914a7d6dfaab4b264480d169efa9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"MLR: Inference + Model conditions\"\nauthor: \"Prof. Maria Tackett\"\nfooter: \"[üîó STA 210 - Spring 2025](https://sta210-sp25.netlify.app)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs: \n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    include-before: [ '<script type=\"text/x-mathjax-config\">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']\n  html: \n    output-file: 10-mlr-inference-conditions-notes.html\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n\n\n\n\n\n## Announcements\n\n-   HW 02 due TODAY at 11:59pm\n\n-   Lab 03 due Thursday, February 13 at 11:59pm\n\n-   Project topics due Sunday, February 16 at 11:59pm\n\n-   [Statistics experience](../hw/stats-experience.html) due Tuesday, April 15\n\n## Exam 01\n\n-   50 points total\n    -   in-class: 35-40 points\n\n    -   take-home: 10 - 15 points\n-   In-class (35 -40 pts): 75 minutes during February 18 lecture\n-   Take-home (10 -15 pts): released after class on Tuesday\n-   If you miss any part of the exam for an excused absence (with academic dean‚Äôs note or other official documentation), your Exam 02 score will be counted twice\n\n## Tips for studying \n\n-   Review exercises in AEs and assignments, asking ‚Äúwhy‚Äù as you review your process and reasoning\n\n    -   e.g., Why do we include ‚Äúholding all else constant‚Äù in interpretations?\n\n-   Focus on understanding not memorization\n\n-   Explain concepts / process to others\n\n-   Ask questions in office hours\n\n-   Review lecture recordings as needed\n\n## Topics\n\n::: nonincremental\n-   Model comparison AE\n-   Inference for multiple linear regression\n-   Checking model conditions\n:::\n\n## Computational setup\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(countdown)\nlibrary(rms)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))\n```\n:::\n\n\n\n\n# Model comparison\n\n## RMSE\n\n$$\nRMSE = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{n}} = \\sqrt{\\frac{\\sum_{i=1}^ne_i^2}{n}}\n$$\n\n::: incremental\n-   Ranges between 0 (perfect predictor) and infinity (terrible predictor)\n\n-   Same units as the response variable\n\n-   The value of RMSE is more useful for comparing across models than evaluating a single model\n:::\n\n## $R^2$ and Adjusted $R^2$\n\n$$R^2 = \\frac{SSM}{SST} = 1 - \\frac{SSR}{SST}$$\n\n<br>\n\n. . .\n\n$$Adj. R^2 = 1 - \\frac{SSR/(n-p-1)}{SST/(n-1)}$$\n\nwhere\n\n-   $n$ is the number of observations used to fit the model\n\n-   $p$ is the number of terms (not including the intercept) in the model\n\n# Application exercise\n\n::: appex\nüìã [sta210-sp25.netlify.app/ae/ae-06-model-compare](../ae/ae-06-model-compare.html)\n:::\n\n<br>\n\n[Click here](https://docs.google.com/presentation/d/16rXxcbXq2ycI7LpxMNmyclBwkdxbhQY_GS4GYX316bE/edit?usp=sharing) to share your group's response.\n\n# Inference for multiple linear regression\n\n## Modeling workflow\n\n-   Split data into training and test sets.\n\n-   Fit, evaluate, and compare candidate models. Choose a final model based on summary of cross validation results.\n\n-   Refit the model using the entire training set and do \"final\" evaluation on the test set (make sure you have not overfit the model).\n\n    -   Adjust as needed if there is evidence of overfit.\n\n-   Use model fit on training set for inference and prediction.\n\n## Data: `rail_trail` {.midi}\n\n::: nonincremental\n-   The Pioneer Valley Planning Commission (PVPC) collected data for ninety days from April 5, 2005 to November 15, 2005.\n-   Data collectors set up a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station.\n:::\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8 √ó 7\n  volume hightemp avgtemp season cloudcover precip day_type\n   <dbl>    <dbl>   <dbl> <chr>       <dbl>  <dbl> <chr>   \n1    501       83    66.5 Summer       7.60 0      Weekday \n2    419       73    61   Summer       6.30 0.290  Weekday \n3    397       74    63   Spring       7.5  0.320  Weekday \n4    385       95    78   Summer       2.60 0      Weekend \n5    200       44    48   Spring      10    0.140  Weekday \n6    375       69    61.5 Spring       6.60 0.0200 Weekday \n7    417       66    52.5 Spring       2.40 0      Weekday \n8    629       66    52   Spring       0    0      Weekend \n```\n\n\n:::\n:::\n\n\n\n\nSource: [Pioneer Valley Planning Commission](http://www.fvgreenway.org/pdfs/Northampton-Bikepath-Volume-Counts%20_05_LTA.pdf) via the **mosaicData** package.\n\n## Variables {.midi}\n\n**Response**:\n\n`volume` estimated number of trail users that day (number of breaks recorded)\n\n. . .\n\n**Predictors**\n\n::: nonincremental\n-   `hightemp` daily high temperature (in degrees Fahrenheit)\n-   `avgtemp` average of daily low and daily high temperature (in degrees Fahrenheit)\n-   `season` one of \"Fall\", \"Spring\", or \"Summer\"\n-   `cloudcover` measure of cloud cover (in oktas)\n-   `precip` measure of precipitation (in inches)\n-   `day_type` one of \"weekday\" or \"weekend\"\n:::\n\n# Conduct a hypothesis test for $\\beta_j$\n\n## Review: Simple linear regression (SLR)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-mlr-inference-conditions_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n\n\n## SLR model summary\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrt_slr_fit <- lm(volume ~ hightemp, data = rail_trail)\n\ntidy(rt_slr_fit) |> kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n|term        | estimate| std.error| statistic| p.value|\n|:-----------|--------:|---------:|---------:|-------:|\n|(Intercept) |   -17.08|     59.40|     -0.29|    0.77|\n|hightemp    |     5.70|      0.85|      6.72|    0.00|\n\n\n:::\n:::\n\n\n\n\n## SLR hypothesis test {.midi}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|term        | estimate| std.error| statistic| p.value|\n|:-----------|--------:|---------:|---------:|-------:|\n|(Intercept) |   -17.08|     59.40|     -0.29|    0.77|\n|hightemp    |     5.70|      0.85|      6.72|    0.00|\n\n\n:::\n:::\n\n\n\n\n1.  **Set hypotheses:** $H_0: \\beta_1 = 0$ vs. $H_a: \\beta_1 \\ne 0$\n\n. . .\n\n2.  **Calculate test statistic and p-value:** The test statistic is $t= 6.72$ . The p-value is calculated using a $t$ distribution with 88 degrees of freedom. The p-value is $\\approx 0$ .\n\n. . .\n\n3.  **State the conclusion:** The p-value is small, so we reject $H_0$. The data provide strong evidence that high temperature is a helpful predictor for the number of daily riders, i.e. there is a linear relationship between high temperature and number of daily riders.\n\n## Multiple linear regression\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrt_mlr_main_fit <- lm(volume ~ hightemp + season, data = rail_trail)\n\ntidy(rt_mlr_main_fit) |> kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n|term         | estimate| std.error| statistic| p.value|\n|:------------|--------:|---------:|---------:|-------:|\n|(Intercept)  |  -125.23|     71.66|     -1.75|    0.08|\n|hightemp     |     7.54|      1.17|      6.43|    0.00|\n|seasonSpring |     5.13|     34.32|      0.15|    0.88|\n|seasonSummer |   -76.84|     47.71|     -1.61|    0.11|\n\n\n:::\n:::\n\n\n\n\n## Multiple linear regression\n\nThe multiple linear regression model assumes $$Y|X_1, X_2,  \\ldots, X_p \\sim N(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p, \\sigma_\\epsilon^2)$$\n\n<br>\n\n. . .\n\nFor a given observation $(x_{i1}, x_{i2}, \\ldots, x_{ip}, y_i)$, we can rewrite the previous statement as\n\n$$y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\epsilon_{i} \\hspace{10mm} \\epsilon_i \\sim N(0,\\sigma_{\\epsilon}^2)$$\n\n------------------------------------------------------------------------\n\n## Estimating $\\sigma_\\epsilon$ {.midi}\n\nFor a given observation $(x_{i1}, x_{i2}, \\ldots,x_{ip}, y_i)$ the residual is $$e_i = y_{i} - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i1} + \\hat{\\beta}_{2} x_{i2} + \\dots + \\hat{\\beta}_p x_{ip})$$\n\n<br>\n\n. . .\n\nThe estimated value of the regression standard error , $\\sigma_{\\epsilon}$, is\n\n$$\\hat{\\sigma}_\\epsilon  = \\sqrt{\\frac{\\sum_{i=1}^ne_i^2}{n-p-1}}$$\n\n. . .\n\nAs with SLR, we use $\\hat{\\sigma}_{\\epsilon}$ to calculate $SE(\\hat{\\beta}_j)$, the standard error of the coefficient for predictor $x_j$. See [Matrix Form of Linear Regression](https://github.com/STA210-Sp19/supplemental-notes/blob/master/regression-basics-matrix.pdf) for more detail.\n\n## MLR hypothesis test: hightemp {.midi}\n\n1.  **Set hypotheses:** $H_0: \\beta_{hightemp} = 0$ vs. $H_a: \\beta_{hightemp} \\ne 0$, given `season` is in the model\n\n. . .\n\n2.  **Calculate test statistic and p-value:** The test statistic is $t = 6.43$. The p-value is calculated using a $t$ distribution with 86 $(n - p - 1)$ degrees of freedom. The p-value is $\\approx 0$.\n\n. . .\n\n3.  **State the conclusion:** The p-value is small, so we reject $H_0$. The data provide strong evidence that high temperature for the day is a useful predictor in a model that already contains the season as a predictor for number of daily riders.\n\n## Interaction terms {.midi}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|term                  | estimate| std.error| statistic| p.value|\n|:---------------------|--------:|---------:|---------:|-------:|\n|(Intercept)           |   -10.53|    166.80|     -0.06|    0.95|\n|hightemp              |     5.48|      2.95|      1.86|    0.07|\n|seasonSpring          |  -293.95|    190.33|     -1.54|    0.13|\n|seasonSummer          |   354.18|    255.08|      1.39|    0.17|\n|hightemp:seasonSpring |     4.88|      3.26|      1.50|    0.14|\n|hightemp:seasonSummer |    -4.54|      3.75|     -1.21|    0.23|\n\n\n:::\n:::\n\n\n\n\n<br>\n\n::: question\nDo the data provide evidence of a significant interaction effect? Comment on the significance of the interaction terms.\n:::\n\n# Confidence interval for $\\beta_j$\n\n## Confidence interval for $\\beta_j$ {.midi}\n\n::: incremental\n-   The $C\\%$ confidence interval for $\\beta_j$ $$\\hat{\\beta}_j \\pm t^* SE(\\hat{\\beta}_j)$$ where $t^*$ follows a $t$ distribution with $n - p - 1$ degrees of freedom.\n-   **Generically**: We are $C\\%$ confident that the interval LB to UB contains the population coefficient of $x_j$.\n-   **In context:** We are $C\\%$ confident that for every one unit increase in $x_j$, $y$ changes by LB to UB units, on average, holding all else constant.\n:::\n\n## Confidence interval for $\\beta_j$\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy(rt_mlr_main_fit, conf.int = TRUE, conf.level = 0.95) |>\n  kable(digits= 2)\n```\n\n::: {.cell-output-display}\n\n\n|term         | estimate| std.error| statistic| p.value| conf.low| conf.high|\n|:------------|--------:|---------:|---------:|-------:|--------:|---------:|\n|(Intercept)  |  -125.23|     71.66|     -1.75|    0.08|  -267.68|     17.22|\n|hightemp     |     7.54|      1.17|      6.43|    0.00|     5.21|      9.87|\n|seasonSpring |     5.13|     34.32|      0.15|    0.88|   -63.10|     73.36|\n|seasonSummer |   -76.84|     47.71|     -1.61|    0.11|  -171.68|     18.00|\n\n\n:::\n:::\n\n\n\n\n## CI for `hightemp` {.midi}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|term         | estimate| std.error| statistic| p.value| conf.low| conf.high|\n|:------------|--------:|---------:|---------:|-------:|--------:|---------:|\n|(Intercept)  |  -125.23|     71.66|     -1.75|    0.08|  -267.68|     17.22|\n|hightemp     |     7.54|      1.17|      6.43|    0.00|     5.21|      9.87|\n|seasonSpring |     5.13|     34.32|      0.15|    0.88|   -63.10|     73.36|\n|seasonSummer |   -76.84|     47.71|     -1.61|    0.11|  -171.68|     18.00|\n\n\n:::\n:::\n\n\n\n\n<br>\n\nWe are 95% confident that for every degree Fahrenheit the day is warmer, the number of riders increases by 5.21 to 9.87, on average, holding season constant.\n\n## CI for `seasonSpring` {.midi}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|term         | estimate| std.error| statistic| p.value| conf.low| conf.high|\n|:------------|--------:|---------:|---------:|-------:|--------:|---------:|\n|(Intercept)  |  -125.23|     71.66|     -1.75|    0.08|  -267.68|     17.22|\n|hightemp     |     7.54|      1.17|      6.43|    0.00|     5.21|      9.87|\n|seasonSpring |     5.13|     34.32|      0.15|    0.88|   -63.10|     73.36|\n|seasonSummer |   -76.84|     47.71|     -1.61|    0.11|  -171.68|     18.00|\n\n\n:::\n:::\n\n\n\n\n<br>\n\nWe are 95% confident that the number of riders on a Spring day is lower by 63.1 to higher by 73.4 compared to a Fall day, on average, holding high temperature for the day constant.\n\n. . .\n\n::: question\nIs `season` a significant predictor of the number of riders, after accounting for high temperature?\n:::\n\n# Inference pitfalls\n\n## Large sample sizes\n\n<br>\n\n:::: callout-caution\nIf the sample size is large enough, the test will likely result in rejecting $H_0: \\beta_j = 0$ even $x_j$ has a very small effect on $y$.\n\n::: nonincremental\n-   Consider the **practical significance** of the result not just the statistical significance.\n\n-   Use the confidence interval to draw conclusions instead of relying only p-values.\n:::\n::::\n\n## Small sample sizes\n\n<br>\n\n:::: callout-caution\nIf the sample size is small, there may not be enough evidence to reject $H_0: \\beta_j=0$.\n\n::: nonincremental\n-   When you fail to reject the null hypothesis, **DON'T** immediately conclude that the variable has no association with the response.\n\n-   There may be a linear association that is just not strong enough to detect given your data, or there may be a non-linear association.\n:::\n::::\n\n# Conditions for inference\n\n## Full model {.smaller}\n\nIncluding all available predictors\n\n**Fit:**\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrt_full_fit <- lm(volume ~ . , data = rail_trail)\n```\n:::\n\n\n\n\n. . .\n\n**Summarize:**\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy(rt_full_fit) |> kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n|term            | estimate| std.error| statistic| p.value|\n|:---------------|--------:|---------:|---------:|-------:|\n|(Intercept)     |    17.62|     76.58|      0.23|    0.82|\n|hightemp        |     7.07|      2.42|      2.92|    0.00|\n|avgtemp         |    -2.04|      3.14|     -0.65|    0.52|\n|seasonSpring    |    35.91|     32.99|      1.09|    0.28|\n|seasonSummer    |    24.15|     52.81|      0.46|    0.65|\n|cloudcover      |    -7.25|      3.84|     -1.89|    0.06|\n|precip          |   -95.70|     42.57|     -2.25|    0.03|\n|day_typeWeekend |    35.90|     22.43|      1.60|    0.11|\n\n\n:::\n:::\n\n\n\n\n## Full model\n\n**Augment:**\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrt_full_aug <- augment(rt_full_fit)\n```\n:::\n\n\n\n\n## Model conditions\n\n1.  **Linearity:** There is a linear relationship between the response and predictor variables.\n\n2.  **Constant Variance:** The variability about the least squares line is generally constant.\n\n3.  **Normality:** The distribution of the residuals is approximately normal.\n\n4.  **Independence:** The residuals are independent from each other.\n\n## Checking Linearity\n\n-   Look at a plot of the residuals vs. predicted values\n\n-   Look at a plot of the residuals vs. each predictor\n\n-   Linearity is met if there is no discernible pattern in each of these plots\n\n    -   e.g., you cannot confidently say if the model under or over predicts for a given fitted value (or range of fitted values)\n\n## Residuals vs. predicted values\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data = rt_full_aug, aes(x = .fitted, y = .resid)) +\n  geom_point(alpha = 0.7) +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(x = \"Predicted values\", y = \"Residuals\")\n```\n\n::: {.cell-output-display}\n![](10-mlr-inference-conditions_files/figure-revealjs/main_res_pred-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n\n\n## Residuals vs. each predictor\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-mlr-inference-conditions_files/figure-revealjs/unnamed-chunk-16-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n\n\n## Checking linearity\n\n-   The plot of the residuals vs. predicted values looked OK\n\n-   The plots of residuals vs. `hightemp` and `avgtemp` appear to have a parabolic pattern.\n\n-   Potential violation in the linearity condition.\n\n. . .\n\n::: question\nGiven this conclusion, what might be a next step in the analysis?\n:::\n\n## Consider adding quadratic terms  {.smaller}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrt_full_fit_2 <- lm(volume ~ . + I(hightemp^2) + I(avgtemp^2), data = rail_trail)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|term            | estimate| std.error| statistic| p.value| conf.low| conf.high|\n|:---------------|--------:|---------:|---------:|-------:|--------:|---------:|\n|(Intercept)     |  -733.08|    249.83|     -2.93|    0.00| -1230.26|   -235.90|\n|hightemp        |    13.33|     10.86|      1.23|    0.22|    -8.29|     34.94|\n|avgtemp         |    18.49|     13.77|      1.34|    0.18|    -8.92|     45.90|\n|seasonSpring    |     1.31|     33.87|      0.04|    0.97|   -66.10|     68.73|\n|seasonSummer    |    24.51|     50.49|      0.49|    0.63|   -75.97|    124.99|\n|cloudcover      |    -8.06|      3.68|     -2.19|    0.03|   -15.39|     -0.74|\n|precip          |   -86.13|     42.71|     -2.02|    0.05|  -171.12|     -1.13|\n|day_typeWeekend |    43.46|     21.58|      2.01|    0.05|     0.51|     86.41|\n|I(hightemp^2)   |    -0.05|      0.08|     -0.65|    0.52|    -0.22|      0.11|\n|I(avgtemp^2)    |    -0.17|      0.12|     -1.37|    0.17|    -0.41|      0.07|\n\n\n:::\n:::\n\n\n\n\n::: question\nIs there evidence of a statistically significant quadratic effect?\n:::\n\n## Checking constant variance\n\n::: question\nDoes the constant variance condition appear to be satisfied?\n:::\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-mlr-inference-conditions_files/figure-revealjs/unnamed-chunk-19-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n\n\n## Checking constant variance\n\n-   The vertical spread of the residuals is not constant across the plot.\n\n-   The constant variance condition is not satisfied.\n\n. . .\n\n::: question\nWhat are the implications for our analysis results? We will talk about how to address this in an upcoming class.\n:::\n\n## Checking normality\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-mlr-inference-conditions_files/figure-revealjs/unnamed-chunk-20-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n\n\nThe distribution of the residuals is approximately unimodal and symmetric, so the normality condition is satisfied. The sample size 90 is sufficiently large to relax this condition if it was not satisfied.\n\n## Checking independence\n\n-   We can often check the independence condition based on the context of the data and how the observations were collected.\n\n-   If the data were collected in a particular order, examine a scatterplot of the residuals versus order in which the data were collected.\n\n-   If there is a grouping variable lurking in the background, check the residuals based on that grouping variable.\n\n## Checking independence {.midi}\n\nResiduals vs. order of data collection:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(rt_full_aug, aes(y = .resid, x = 1:nrow(rt_full_aug))) +\n  geom_line() +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(x = \"Order of data collection\", y = \"Residuals\")\n```\n\n::: {.cell-output-display}\n![](10-mlr-inference-conditions_files/figure-revealjs/unnamed-chunk-21-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n\n\n## Checking independence\n\n-   No clear pattern in the residuals vs. order of data collection plot.\n\n-   Independence condition appears to be satisfied, as far as we can evaluate it.\n\n## Recap\n\n-   Reviewed model comparison\n-   Introduced inference for multiple linear regression\n-   Checked model conditions for linear regression\n\n## Next class\n\n-   Exam 01 review\n",
    "supporting": [
      "10-mlr-inference-conditions_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}