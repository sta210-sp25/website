{
  "hash": "85ef6ac0193ac3c655ee26d4720b976a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"MLR: Model comparison + Inference\"\nauthor: \"Prof. Maria Tackett\"\nfooter: \"[🔗 STA 210 - Spring 2025](https://sta210-sp25.netlify.app)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs: \n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    include-before: [ '<script type=\"text/x-mathjax-config\">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']\n  html: \n    output-file: 10-mlr-inference-conditions-notes.html\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n\n\n\n\n\n\n\n\n\n## Announcements\n\n-   HW 02 due TODAY at 11:59pm\n\n-   Lab 03 due Thursday, February 13 at 11:59pm\n\n-   Project topics due Sunday, February 16 at 11:59pm\n\n-   [Statistics experience](../hw/stats-experience.html) due Tuesday, April 15\n\n## Exam 01\n\n-   50 points total\n    -   in-class: 35-40 points\n\n    -   take-home: 10 - 15 points\n-   In-class (35 -40 pts): 75 minutes during February 18 lecture\n-   Take-home (10 -15 pts): released after class on Tuesday\n-   If you miss any part of the exam for an excused absence (with academic dean’s note or other official documentation), your Exam 02 score will be counted twice\n\n## Tips for studying\n\n-   Review exercises in AEs and assignments, asking “why” as you review your process and reasoning\n\n    -   e.g., Why do we include “holding all else constant” in interpretations?\n\n-   Focus on understanding not memorization\n\n-   Explain concepts / process to others\n\n-   Ask questions in office hours\n\n-   Review lecture recordings as needed\n\n## Topics\n\n::: nonincremental\n-   Model comparison AE\n-   Inference for multiple linear regression\n:::\n\n## Computational setup\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# load packages\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.6     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(countdown)\nlibrary(rms)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: Hmisc\n\nAttaching package: 'Hmisc'\n\nThe following object is masked from 'package:parsnip':\n\n    translate\n\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n```\n\n\n:::\n\n```{.r .cell-code}\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))\n```\n:::\n\n\n\n\n\n\n\n\n# Model comparison\n\n## RMSE\n\n$$\nRMSE = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{n}} = \\sqrt{\\frac{\\sum_{i=1}^ne_i^2}{n}}\n$$\n\n::: incremental\n-   Ranges between 0 (perfect predictor) and infinity (terrible predictor)\n\n-   Same units as the response variable\n\n-   The value of RMSE is more useful for comparing across models than evaluating a single model\n:::\n\n## $R^2$ and Adjusted $R^2$\n\n$$R^2 = \\frac{SSM}{SST} = 1 - \\frac{SSR}{SST}$$\n\n<br>\n\n. . .\n\n$$Adj. R^2 = 1 - \\frac{SSR/(n-p-1)}{SST/(n-1)}$$\n\nwhere\n\n-   $n$ is the number of observations used to fit the model\n\n-   $p$ is the number of terms (not including the intercept) in the model\n\n# Application exercise\n\n::: appex\n📋 [sta210-sp25.netlify.app/ae/ae-06-model-compare](../ae/ae-06-model-compare.html)\n:::\n\n<br>\n\n[Click here](https://docs.google.com/presentation/d/16rXxcbXq2ycI7LpxMNmyclBwkdxbhQY_GS4GYX316bE/edit?usp=sharing) to share your group's response.\n\n# Inference for multiple linear regression\n\n## Modeling workflow\n\n-   Split data into training and test sets.\n\n-   Fit, evaluate, and compare candidate models. Choose a final model based on summary of cross validation results.\n\n-   Refit the model using the entire training set and do \"final\" evaluation on the test set (make sure you have not overfit the model).\n\n    -   Adjust as needed if there is evidence of overfit.\n\n-   Use model fit on training set for inference and prediction.\n\n## Data: `rail_trail` {.midi}\n\n::: nonincremental\n-   The Pioneer Valley Planning Commission (PVPC) collected data for ninety days from April 5, 2005 to November 15, 2005.\n-   Data collectors set up a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station.\n:::\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 90 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): season, day_type\ndbl (5): volume, hightemp, avgtemp, cloudcover, precip\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8 × 7\n  volume hightemp avgtemp season cloudcover precip day_type\n   <dbl>    <dbl>   <dbl> <chr>       <dbl>  <dbl> <chr>   \n1    501       83    66.5 Summer       7.60 0      Weekday \n2    419       73    61   Summer       6.30 0.290  Weekday \n3    397       74    63   Spring       7.5  0.320  Weekday \n4    385       95    78   Summer       2.60 0      Weekend \n5    200       44    48   Spring      10    0.140  Weekday \n6    375       69    61.5 Spring       6.60 0.0200 Weekday \n7    417       66    52.5 Spring       2.40 0      Weekday \n8    629       66    52   Spring       0    0      Weekend \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nSource: [Pioneer Valley Planning Commission](http://www.fvgreenway.org/pdfs/Northampton-Bikepath-Volume-Counts%20_05_LTA.pdf) via the **mosaicData** package.\n\n## Variables {.midi}\n\n**Response**:\n\n`volume` estimated number of trail users that day (number of breaks recorded)\n\n. . .\n\n**Predictors**\n\n::: nonincremental\n-   `hightemp` daily high temperature (in degrees Fahrenheit)\n-   `avgtemp` average of daily low and daily high temperature (in degrees Fahrenheit)\n-   `season` one of \"Fall\", \"Spring\", or \"Summer\"\n-   `cloudcover` measure of cloud cover (in oktas)\n-   `precip` measure of precipitation (in inches)\n-   `day_type` one of \"weekday\" or \"weekend\"\n:::\n\n# Conduct a hypothesis test for $\\beta_j$\n\n## Review: Simple linear regression (SLR)\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](10-mlr-inference-conditions_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n\n\n\n\n\n\n## SLR model summary\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrt_slr_fit <- lm(volume ~ hightemp, data = rail_trail)\n\ntidy(rt_slr_fit) |> kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n|term        | estimate| std.error| statistic| p.value|\n|:-----------|--------:|---------:|---------:|-------:|\n|(Intercept) |   -17.08|     59.40|     -0.29|    0.77|\n|hightemp    |     5.70|      0.85|      6.72|    0.00|\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n## SLR hypothesis test {.midi}\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|term        | estimate| std.error| statistic| p.value|\n|:-----------|--------:|---------:|---------:|-------:|\n|(Intercept) |   -17.08|     59.40|     -0.29|    0.77|\n|hightemp    |     5.70|      0.85|      6.72|    0.00|\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n1.  **Set hypotheses:** $H_0: \\beta_1 = 0$ vs. $H_a: \\beta_1 \\ne 0$\n\n. . .\n\n2.  **Calculate test statistic and p-value:** The test statistic is $t= 6.72$ . The p-value is calculated using a $t$ distribution with 88 degrees of freedom. The p-value is $\\approx 0$ .\n\n. . .\n\n3.  **State the conclusion:** The p-value is small, so we reject $H_0$. The data provide strong evidence that high temperature is a helpful predictor for the number of daily riders, i.e. there is a linear relationship between high temperature and number of daily riders.\n\n## Multiple linear regression\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrt_mlr_main_fit <- lm(volume ~ hightemp + season, data = rail_trail)\n\ntidy(rt_mlr_main_fit) |> kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n|term         | estimate| std.error| statistic| p.value|\n|:------------|--------:|---------:|---------:|-------:|\n|(Intercept)  |  -125.23|     71.66|     -1.75|    0.08|\n|hightemp     |     7.54|      1.17|      6.43|    0.00|\n|seasonSpring |     5.13|     34.32|      0.15|    0.88|\n|seasonSummer |   -76.84|     47.71|     -1.61|    0.11|\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n## Multiple linear regression\n\nThe multiple linear regression model assumes $$Y|X_1, X_2,  \\ldots, X_p \\sim N(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p, \\sigma_\\epsilon^2)$$\n\n<br>\n\n. . .\n\nFor a given observation $(x_{i1}, x_{i2}, \\ldots, x_{ip}, y_i)$, we can rewrite the previous statement as\n\n$$y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\epsilon_{i} \\hspace{10mm} \\epsilon_i \\sim N(0,\\sigma_{\\epsilon}^2)$$\n\n------------------------------------------------------------------------\n\n## Estimating $\\sigma_\\epsilon$ {.midi}\n\nFor a given observation $(x_{i1}, x_{i2}, \\ldots,x_{ip}, y_i)$ the residual is $$e_i = y_{i} - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i1} + \\hat{\\beta}_{2} x_{i2} + \\dots + \\hat{\\beta}_p x_{ip})$$\n\n<br>\n\n. . .\n\nThe estimated value of the regression standard error , $\\sigma_{\\epsilon}$, is\n\n$$\\hat{\\sigma}_\\epsilon  = \\sqrt{\\frac{\\sum_{i=1}^ne_i^2}{n-p-1}}$$\n\n. . .\n\nAs with SLR, we use $\\hat{\\sigma}_{\\epsilon}$ to calculate $SE(\\hat{\\beta}_j)$, the standard error of the coefficient for predictor $x_j$. See [Matrix Form of Linear Regression](https://github.com/STA210-Sp19/supplemental-notes/blob/master/regression-basics-matrix.pdf) for more detail.\n\n## MLR hypothesis test: hightemp {.midi}\n\n1.  **Set hypotheses:** $H_0: \\beta_{hightemp} = 0$ vs. $H_a: \\beta_{hightemp} \\ne 0$, given `season` is in the model\n\n. . .\n\n2.  **Calculate test statistic and p-value:** The test statistic is $t = 6.43$. The p-value is calculated using a $t$ distribution with 86 $(n - p - 1)$ degrees of freedom. The p-value is $\\approx 0$.\n\n. . .\n\n3.  **State the conclusion:** The p-value is small, so we reject $H_0$. The data provide strong evidence that high temperature for the day is a useful predictor in a model that already contains the season as a predictor for number of daily riders.\n\n## Interaction terms {.midi}\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|term                  | estimate| std.error| statistic| p.value|\n|:---------------------|--------:|---------:|---------:|-------:|\n|(Intercept)           |   -10.53|    166.80|     -0.06|    0.95|\n|hightemp              |     5.48|      2.95|      1.86|    0.07|\n|seasonSpring          |  -293.95|    190.33|     -1.54|    0.13|\n|seasonSummer          |   354.18|    255.08|      1.39|    0.17|\n|hightemp:seasonSpring |     4.88|      3.26|      1.50|    0.14|\n|hightemp:seasonSummer |    -4.54|      3.75|     -1.21|    0.23|\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n<br>\n\n::: question\nDo the data provide evidence of a significant interaction effect? Comment on the significance of the interaction terms.\n:::\n\n# Confidence interval for $\\beta_j$\n\n## Confidence interval for $\\beta_j$ {.midi}\n\n::: incremental\n-   The $C\\%$ confidence interval for $\\beta_j$ $$\\hat{\\beta}_j \\pm t^* SE(\\hat{\\beta}_j)$$ where $t^*$ follows a $t$ distribution with $n - p - 1$ degrees of freedom.\n-   **Generically**: We are $C\\%$ confident that the interval LB to UB contains the population coefficient of $x_j$.\n-   **In context:** We are $C\\%$ confident that for every one unit increase in $x_j$, $y$ changes by LB to UB units, on average, holding all else constant.\n:::\n\n## Confidence interval for $\\beta_j$\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy(rt_mlr_main_fit, conf.int = TRUE, conf.level = 0.95) |>\n  kable(digits= 2)\n```\n\n::: {.cell-output-display}\n\n\n|term         | estimate| std.error| statistic| p.value| conf.low| conf.high|\n|:------------|--------:|---------:|---------:|-------:|--------:|---------:|\n|(Intercept)  |  -125.23|     71.66|     -1.75|    0.08|  -267.68|     17.22|\n|hightemp     |     7.54|      1.17|      6.43|    0.00|     5.21|      9.87|\n|seasonSpring |     5.13|     34.32|      0.15|    0.88|   -63.10|     73.36|\n|seasonSummer |   -76.84|     47.71|     -1.61|    0.11|  -171.68|     18.00|\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n## CI for `hightemp` {.midi}\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|term         | estimate| std.error| statistic| p.value| conf.low| conf.high|\n|:------------|--------:|---------:|---------:|-------:|--------:|---------:|\n|(Intercept)  |  -125.23|     71.66|     -1.75|    0.08|  -267.68|     17.22|\n|hightemp     |     7.54|      1.17|      6.43|    0.00|     5.21|      9.87|\n|seasonSpring |     5.13|     34.32|      0.15|    0.88|   -63.10|     73.36|\n|seasonSummer |   -76.84|     47.71|     -1.61|    0.11|  -171.68|     18.00|\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n<br>\n\nWe are 95% confident that for every degree Fahrenheit the day is warmer, the number of riders increases by 5.21 to 9.87, on average, holding season constant.\n\n## CI for `seasonSpring` {.midi}\n\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|term         | estimate| std.error| statistic| p.value| conf.low| conf.high|\n|:------------|--------:|---------:|---------:|-------:|--------:|---------:|\n|(Intercept)  |  -125.23|     71.66|     -1.75|    0.08|  -267.68|     17.22|\n|hightemp     |     7.54|      1.17|      6.43|    0.00|     5.21|      9.87|\n|seasonSpring |     5.13|     34.32|      0.15|    0.88|   -63.10|     73.36|\n|seasonSummer |   -76.84|     47.71|     -1.61|    0.11|  -171.68|     18.00|\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n<br>\n\nWe are 95% confident that the number of riders on a Spring day is lower by 63.1 to higher by 73.4 compared to a Fall day, on average, holding high temperature for the day constant.\n\n. . .\n\n::: question\nIs `season` a significant predictor of the number of riders, after accounting for high temperature?\n:::\n\n# Inference pitfalls\n\n## Large sample sizes\n\n<br>\n\n:::: callout-caution\nIf the sample size is large enough, the test will likely result in rejecting $H_0: \\beta_j = 0$ even $x_j$ has a very small effect on $y$.\n\n::: nonincremental\n-   Consider the **practical significance** of the result not just the statistical significance.\n\n-   Use the confidence interval to draw conclusions instead of relying only p-values.\n:::\n::::\n\n## Small sample sizes\n\n<br>\n\n:::: callout-caution\nIf the sample size is small, there may not be enough evidence to reject $H_0: \\beta_j=0$.\n\n::: nonincremental\n-   When you fail to reject the null hypothesis, **DON'T** immediately conclude that the variable has no association with the response.\n\n-   There may be a linear association that is just not strong enough to detect given your data, or there may be a non-linear association.\n:::\n::::\n\n## Recap\n\n-   Reviewed model comparison\n-   Introduced inference for multiple linear regression\n\n## Next class\n\n-   Exam 01 review\n",
    "supporting": [
      "10-mlr-inference-conditions_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}