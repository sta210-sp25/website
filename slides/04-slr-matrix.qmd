---
title: "SLR: Matrix representation"
author: "Prof. Maria Tackett"
date: "2024-09-05"
date-format: "MMM DD, YYYY"
footer: "[ðŸ”— STA 221 - Fall 2024](https://sta221-fa24.netlify.app)"
logo: "../images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: true
    incremental: false 
    chalkboard: true
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
execute:
  freeze: auto
  echo: true
knitr:
  opts_chunk: 
    R.options:      
    width: 200
bibliography: references.bib
---

```{r setup}
#| include: false

library(countdown)

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = 0.618,
  fig.retina = 3,
  dpi = 300,
  out.width = "80%",
  fig.align = "center"
)
```

## Topics

-   Matrix representation for simple linear regression
    -   Model from
    -   Least square estimate
    -   Fitted (predicted) values
    -   Residuals
-   Matrix representation in R

## Computing set up

```{r packages}
#| echo: true
#| message: false

# load packages
library(tidyverse)   # for data wrangling and visualization
library(tidymodels)  # for modeling (includes broom, yardstick, and other packages)
library(openintro)   # for the duke_forest dataset
library(scales)      # for pretty axis labels
library(knitr)       # for pretty tables
library(patchwork)   # arrange plots

# set default theme for ggplot2
ggplot2::theme_set(ggplot2::theme_bw())
```

# Matrix form for simple linear regression

## SLR: Statistical model (population)

When we have a quantitative response, $Y$, and a single quantitative predictor, $X$, we can use a **simple linear regression** model to describe the relationship between $Y$ and $X$. $$\large{Y = \mathbf{\beta_0 + \beta_1 X} + \epsilon}, \hspace{8mm} \epsilon \sim N(0, \sigma_{\epsilon}^2)$$

<br>

-   $\beta_1$: Population (true) slope of the relationship between $X$ and $Y$
-   $\beta_0$: Population (true) intercept of the relationship between $X$ and $Y$
-   $\epsilon$: Error

## SLR in matrix form

Suppose we have $n$ observations.

$$
\underbrace{
\begin{bmatrix}
y_1 \\
\vdots \\
y_n
\end{bmatrix} }_
{\mathbf{y}} \hspace{3mm}
= 
\hspace{3mm}
\underbrace{
\begin{bmatrix}
1 & \dots &x_1 \\
\vdots & \ddots & \vdots \\
1 & \dots & x_n
\end{bmatrix}
}_{\mathbf{X}}
\hspace{2mm}
\underbrace{
\begin{bmatrix}
\beta_0 \\
\beta_1
\end{bmatrix}
}_{\boldsymbol{\beta}}
\hspace{3mm}
+
\hspace{3mm}
\underbrace{
\begin{bmatrix}
\epsilon_1 \\
\vdots\\
\epsilon_n
\end{bmatrix}
}_\boldsymbol{\epsilon}
$$

<br>

::: question
What are the dimensions of $\mathbf{y}$, $\mathbf{X}$, $\boldsymbol{\beta}$, and $\boldsymbol{\epsilon}$?
:::

## Sum of squared residuals

We will use the sum of squared residuals (also called "sum of squared error") to find the least squares line:

$$
SSR = \sum_{i=1}^ne_i^2 = \mathbf{e}^T\mathbf{e} = (\mathbf{y} - \hat{\mathbf{y}})^T(\mathbf{y} - \hat{\mathbf{y}})
$$

<br>

::: question
-   What is the dimension of SSR?

-   What is $\hat{\mathbf{y}}$ in terms of $\mathbf{y}$, $\mathbf{X}$, and/or $\boldsymbol{\beta}$ ?
:::

## Minimize sum of squared residuals

We want to find values of $\boldsymbol{\beta} = \begin{bmatrix}\beta_0 \\ \beta_1 \end{bmatrix}$ that minimize the sum of squared residuals $$
\begin{aligned}
\mathbf{e}^T\mathbf{e} &= (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^T(\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) \\[10pt]
&= (\mathbf{y}^T - \boldsymbol{\beta}^T\mathbf{X}^T)(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})\\[10pt]
&=\mathbf{y}^T\mathbf{y} - \mathbf{y}^T\mathbf{X}\boldsymbol{\beta} - \boldsymbol{\beta}^T\mathbf{X}^T\mathbf{y} + \boldsymbol{\beta}^T\mathbf{X}^T\mathbf{X}\boldsymbol{\beta}\\[10pt]
&=\mathbf{y}^T\mathbf{y} - 2\boldsymbol{\beta}^T\mathbf{X}^T\mathbf{y} + \boldsymbol{\beta}^T\mathbf{X}^T\mathbf{X}\boldsymbol{\beta}
\end{aligned}
$$

## Least squares estimators

$$
SSR = \mathbf{e}^T\mathbf{e} =\mathbf{y}^T\mathbf{y} - 2\boldsymbol{\beta}^T\mathbf{X}^T\mathbf{y} + \boldsymbol{\beta}^T\mathbf{X}^T\mathbf{X}\boldsymbol{\beta}
$$

<br>

. . .

The least squares estimators must satisfy

$$
\frac{\partial SSR}{\partial \boldsymbol{\beta}} = -2\mathbf{X}^T\mathbf{y} + 2\mathbf{X}^T\mathbf{X}\boldsymbol{\beta} = 0
$$

<br>

. . .

$$
\color{#993399}{\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}}
$$

## Matrix form in R

Let's go back to the Duke Forest data. We want to use the matrix representation to fit a model of the form:

$$
price = \beta_0 + \beta_1 ~ area + \epsilon, \hspace{5mm} \epsilon \sim N(0, \sigma^2_\epsilon)
$$

. . .

Get $\mathbf{y}$ the vector of responses

```{r}
y <- duke_forest$price
```

. . .

Let's look at the first 10 observations of $y$

```{r}
y[1:10]
```

## Matrix form in R

Use the `model.matrix()` function to get $\mathbf{X}$

```{r}
X <- model.matrix(price ~ area, data = duke_forest)
```

. . .

Let's look at the first 10 rows of $\mathbf{X}$

```{r}
X[1:10,]
```

## Matrix form in R

Matrix functions in R. Let $\mathbf{A}$ and $\mathbf{B}$ be matrices

-   `t(A)`: transpose $\mathbf{A}$

-   `A %*% B`: multiply $\mathbf{A}$ and $\mathbf{B}$

-   `solve(A)`: inverse of $\mathbf{A}$

. . .

Now let's calculate $\hat{\boldsymbol{\beta}}$

```{r}
beta_hat <- solve(t(X)%*%X)%*%t(X)%*%y
beta_hat
```

## Compare to result from `lm`

```{r}
duke_forest_model <- lm(price ~ area, data = duke_forest)
tidy(duke_forest_model) |> kable(digits = 3)
```

<br>

. . .

```{r}
beta_hat 
```

## Recap

-   Evaluated models using RMSE and $R^2$

-   Used analysis of variance to partition variability in the response variable
