---
title: "Inference for regression"
subtitle: "cont'd"
author: "Prof. Maria Tackett"
date: "2024-09-24"
date-format: "MMM DD, YYYY"
footer: "[ðŸ”— STA 221 - Fall 2024](https://sta221-fa24.netlify.app)"
logo: "../images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: true
    incremental: false 
    chalkboard: true
    include-before: [ '<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
execute:
  freeze: auto
  echo: true
knitr:
  opts_chunk: 
    R.options:      
    width: 200
filters:
  - parse-latex
bibliography: references.bib
---

```{r setup}
#| include: false

library(countdown)

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = 0.618,
  fig.retina = 3,
  dpi = 300,
  out.width = "80%",
  fig.align = "center"
)

options(scipen=999)
```

## Announcements

-   Project

    -   Research questions due **Thursday at 11:59pm**

    -   Proposal due Thursday, October 3 at 11:59pm

-   Lab 03 due Thursday, October 3 at 11:59pm

-   [Statistics experience](https://sta221-fa24.netlify.app/hw/stats-experience) due **Tue, Nov 26 at 11:59pm**

## Topics

-   Understand statistical inference in the context of regression

-   Describe the assumptions for regression

-   Understand connection between distribution of residuals and inferential procedures

-   Conduct inference on a single coefficient

-   Conduct inference on the overall regression model

## Computing setup

```{r packages}
#| echo: true
#| message: false

# load packages
library(tidyverse)  
library(tidymodels)  
library(knitr)       
library(kableExtra)  
library(patchwork)   

# set default theme in ggplot2
ggplot2::theme_set(ggplot2::theme_bw())
```

## Data: NCAA Football expenditures {.midi}

Today's data come from [Equity in Athletics Data Analysis](https://ope.ed.gov/athletics/#/datafile/list) and includes information about sports expenditures and revenues for colleges and universities in the United States. This data set was featured in a [March 2022 Tidy Tuesday](https://github.com/rfordatascience/tidytuesday/blob/master/data/2022/2022-03-29/readme.md).

We will focus on the 2019 - 2020 season expenditures on football for institutions in the NCAA - Division 1 FBS. The variables are :

-   `total_exp_m`: Total expenditures on football in the 2019 - 2020 academic year (in millions USD)

-   `enrollment_th`: Total student enrollment in the 2019 - 2020 academic year (in thousands)

-   `type`: institution type (Public or Private)

```{r}
#| include: false
#| eval: false

## code to make data set for these notes

sports <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-03-29/sports.csv') 

# filter data to only include D1 football for the year 2019

sports |>
  filter(sports == "Football", 
         classification_name == "NCAA Division I-FBS", year == 2019) |>
  mutate(type = if_else(sector_name == "Private nonprofit, 4-year or above", "Private", "Public"), 
         enrollment_th = ef_total_count / 1000,
         total_exp_m = total_exp_menwomen/ 1000000) |>
  select(year, institution_name, city_txt, state_cd, zip_text, type,
         enrollment_th, 
         total_exp_m) |> 
  write_csv("data/ncaa-football-exp.csv")


```

```{r}
football <- read_csv("data/ncaa-football-exp.csv")
```

## Univariate EDA

```{r}
#| echo: false

p1 <- ggplot(data = football, aes(x = total_exp_m)) + 
  geom_histogram(fill = "steelblue", color = "black", binwidth = 5) + 
  labs( x= "Total Football Expenditures (in $Millions)")

p2 <- ggplot(data = football, aes(x = enrollment_th)) + 
  geom_histogram(binwidth = 5, fill = "steelblue", color = "black") +
  labs(x = "Total Student Enrollment (in Thousands)")

p3 <- ggplot(data = football, aes(x = type)) + 
  geom_bar(fill = "steelblue", color = "black") + 
  labs(x = "Insitution Type")

p1 + (p2 / p3)
```

## Bivariate EDA

```{r}
#| echo: false
#| 
p4 <- ggplot(data = football, aes(x = enrollment_th, y = total_exp_m)) +
  geom_point() +
  labs(x = "Total Student Enrollment (in Thousands)", 
       y = "Total Football Expenditures (in $Millions)", 
       title = "Football Expenditures vs. Enrollment")

p5 <- ggplot(data = football, aes(x = type, y = total_exp_m, fill = type)) + 
  geom_boxplot() +
  labs(x = "Institution Type", 
       y = "",
       title = "Football Expenditure vs. Type") + 
  theme(legend.position = "none")

p4 + p5
```

## Regression model

```{r}
#| echo: true
exp_fit <- lm(total_exp_m ~ enrollment_th + type, data = football)
tidy(exp_fit) |>
  kable(digits = 3)
```

<br>

For every additional 1,000 students, we expect the institution's total expenditures on football to increase by \$780,000, on average, holding institution type constant.

# Inference for regression

## Statistical inference {.midi}

::: columns
::: {.column width="40%"}
-   **Statistical inference** provides methods and tools so we can use the single observed sample to make valid statements (inferences) about the population it comes from

-   For our inferences to be valid, the sample should be representative (ideally random) of the population we're interested in
:::

::: {.column width="60%"}
![Image source: Eugene Morgan Â© Penn State](images/08/inference.png){fig-align="center"}
:::
:::

## Inference for linear regression

-   **Inference based on ANOVA**

    -   Hypothesis test for the statistical significance of the overall regression model

    -   Hypothesis test for a subset of coefficients

-   **Inference for a single coefficient** $\beta_j$

    -   Hypothesis test for a coefficient $\beta_j$

    -   Confidence interval for a coefficient $\beta_j$

## Linear regression model {.midi}

$$
\begin{aligned}
\mathbf{y} &= Model + Error \\[5pt]
&= f(\mathbf{X}) + \boldsymbol{\epsilon} \\[5pt]
&= E(\mathbf{y}|\mathbf{X}) + \mathbf{\epsilon} \\[5pt]
&= \mathbf{X}\boldsymbol{\beta} + \mathbf{\epsilon}
\end{aligned}
$$

. . .

::: incremental
-   We have discussed multiple ways to find the least squares estimates of $\boldsymbol{\beta} = \begin{bmatrix}\beta_0 \\\beta_1\end{bmatrix}$

    -   None of these approaches depend on the distribution of $\boldsymbol{\epsilon}$

-   Now we will use statistical inference to draw conclusions about $\boldsymbol{\beta}$ that depend on particular assumptions about the distribution of $\boldsymbol{\epsilon}$
:::

## Linear regression model {.midi}

$$
\mathbf{y}|\mathbf{X} \sim N(\mathbf{X}\boldsymbol{\beta}, \sigma_\epsilon^2\mathbf{I})
$$

![Image source: *Introduction to the Practice of Statistics (5th ed)*](images/08/regression.png){fig-align="center"}

## Expected value of $\mathbf{y}$

Let $\mathbf{b} = \begin{bmatrix}b_1 \\ \vdots \\b_p\end{bmatrix}$ be a $p \times 1$ vector of random variables.

<br>

. . .

Then $E(\mathbf{b}) = E\begin{bmatrix}b_1 \\ \vdots \\ b_p\end{bmatrix} = \begin{bmatrix}E(b_1) \\ \vdots \\ E(b_p)\end{bmatrix}$

<br>

. . .

::: question
Use this to find $E(\mathbf{y}|\mathbf{X})$.
:::

## Variance {.midi}

Let $\mathbf{b} = \begin{bmatrix}b_1 \\ \vdots \\b_p\end{bmatrix}$ be a $p \times 1$ vector of *independent* random variables.

<br>

. . .

Then $Var(\mathbf{b}) = \begin{bmatrix}Var(b_1) & 0 & \dots & 0 \\ 0 & Var(b_2) & \dots & 0 \\ \vdots & \vdots & \dots & \cdot \\ 0 & 0 & \dots & Var(b_p)\end{bmatrix}$

<br>

. . .

::: question
Use this to find $Var(\mathbf{y}|\mathbf{X})$.
:::

## Assumptions of regression {.midi}

::: columns
::: {.column width="50%"}
$$
\mathbf{y}|\mathbf{X} \sim N(\mathbf{X}\boldsymbol{\beta}, \sigma_\epsilon^2\mathbf{I})
$$

![Image source: *Introduction to the Practice of Statistics (5th ed)*](images/08/regression.png){fig-align="center"}
:::

::: {.column width="50%"}
1.  **Linearity:** There is a linear relationship between the response and predictor variables.
2.  **Constant Variance:** The variability about the least squares line is generally constant.
3.  **Normality:** The distribution of the residuals is approximately normal.
4.  **Independence:** The residuals are independent from one another.
:::
:::

## Estimating $\sigma^2_{\epsilon}$ {.midi}

-   Once we fit the model, we can use the residuals to estimate $\sigma_{\epsilon}^2$

-   $\hat{\sigma}^2_{\epsilon}$ is needed for hypothesis testing and constructing confidence intervals for regression

$$
\hat{\sigma}^2_\epsilon = \frac{\sum_\limits{i=1}^n(y_i - \hat{y}_i)^2}{n-p-1} = \frac{\sum_\limits{i=1}^ne_i^2}{n - p - 1} = \frac{SSR}{n - p - 1}
$$

-   The **regression standard error** $\hat{\sigma}_{\epsilon}$ is a measure of the average distance between the observations and regression line

$$
\hat{\sigma}_\epsilon = \sqrt{\frac{SSR}{n - p - 1}} 
$$

# Inference for a single coefficient

## Inference for $\beta_j$

We often want to conduct inference on individual model coefficients

-   **Hypothesis test:** Is there a linear relationship between the response and $x_j$?

-   **Confidence interval**: What is a plausible range of values $\beta_j$ can take?

But first we need to understand the distribution of $\hat{\beta}_j$

## Sampling distribution of $\hat{\beta}_j$

$$
\hat{\boldsymbol{\beta}} \sim N(\boldsymbol{\beta}, \sigma^2_\epsilon(\mathbf{X}^T\mathbf{X})^{-1})
$$

Let $\mathbf{C} = (\mathbf{X}^T\mathbf{X})^{-1}$. Then, for each coefficient $\hat{\beta}_j$,

::: incremental
-   $E(\hat{\beta}_j) = \boldsymbol{\beta}_j$, the $j^{th}$ element of $\boldsymbol{\beta}$

-   $Var(\hat{\beta}_j) = \sigma^2_{\epsilon}C_{jj}$

-   $Cov(\hat{\beta}_i, \hat{\beta}_j) = \sigma^2_{\epsilon}C_{ij}$
:::

# Hypothesis test for $\beta_j$

## Steps for a hypothesis test

1.  State the null and alternative hypotheses.
2.  Calculate a test statistic.
3.  Calculate the p-value.
4.  State the conclusion.

## Hypothesis test for $\beta_j$: Hypotheses

We will generally test the hypotheses:

-   **Null Hypothesis**: $H_0: \beta_j = 0$

    -   There is no linear relationship between $\beta_j$ and $y$ after accounting for the other variables in the model

-   **Alternative hypothesis**: $H_a: \beta_j \neq 0$

    -   There is a linear relationship between $\beta_j$ and $y$ after accounting for the other variables in the model

## Hypothesis test for $\beta_j$: Test statistic {.midi}

**Test statistic:** Number of standard errors the estimate is away from the null hypothesized value

$$
\text{Test Statstic} = \frac{\text{Estimate - Null}}{\text{Standard error}} \\
$$

<br>

. . .

$$T = \frac{\hat{\beta}_j - 0}{SE(\hat{\beta}_j)} ~ = ~\frac{\hat{\beta}_j - 0}{\sqrt{\hat{\sigma}^2_\epsilon C_{jj}}} ~\sim ~ t_{n-p-1}
$$

## Hypothesis test for $\beta_j$: P-value

The **p-value** is the probability of observing a test statistic at least as extreme (in the direction of the alternative hypothesis) from the null value as the one observed

$$
p-value = P(|t| > |\text{test statistic}|),
$$

calculated from a $t$ distribution with $n- p - 1$ degrees of freedom

## Understanding the p-value

| Magnitude of p-value    | Interpretation                        |
|:------------------------|:--------------------------------------|
| p-value \< 0.01         | strong evidence against $H_0$         |
| 0.01 \< p-value \< 0.05 | moderate evidence against $H_0$       |
| 0.05 \< p-value \< 0.1  | weak evidence against $H_0$           |
| p-value \> 0.1          | effectively no evidence against $H_0$ |

**These are general guidelines. The strength of evidence depends on the context of the problem.**

## Hypothesis test for $\beta_j$: Conclusion

**There are two parts to the conclusion**

-   Make a conclusion by comparing the p-value to a predetermined decision-making threshold called the significance level ( $\alpha$ level)

    -   If $\text{p-value} < \alpha$: Reject $H_0$

    -   If $\text{p-value} \geq \alpha$: Fail to reject $H_0$

-   State the conclusion in the context of the data

# Application exercise

::: appex
ðŸ“‹ <https://sta221-fa24.netlify.app/ae/ae-03-inference>
:::

# Confidence interval for $\beta_j$

## Confidence interval for $\beta_j$ {.midi}

::: incremental
-   A plausible range of values for a population parameter is called a **confidence interval**

-   Using only a single point estimate is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net

    -   We can throw a spear where we saw a fish but we will probably miss, if we toss a net in that area, we have a good chance of catching the fish

    -   Similarly, if we report a point estimate, we probably will not hit the exact population parameter, but if we report a range of plausible values we have a good shot at capturing the parameter
:::

## What "confidence" means {.midi}

::: incremental
-   We will construct $C\%$ confidence intervals.

    -   The confidence level impacts the width of the interval

<br>

-   "Confident" means if we were to take repeated samples of the same size as our data, fit regression lines using the same predictors, and calculate $C\%$ CIs for the coefficient of $x_j$, then $C\%$ of those intervals will contain the true value of the coefficient $\beta_j$

<br>

-   Balance precision and accuracy when selecting a confidence level
:::

## Confidence interval for $\beta_j$

$$
\text{Estimate} \pm \text{ (critical value) } \times \text{SE}
$$

<br>

. . .

$$
\hat{\beta}_1 \pm t^* \times SE({\hat{\beta}_j})
$$

where $t^*$ is calculated from a $t$ distribution with $n-p-1$ degrees of freedom

## Confidence interval: Critical value

::: {.fragment fragment-index="1"}
```{r}
#| echo: true

# confidence level: 95%
qt(0.975, df = nrow(football) - 2 - 1)
```
:::

<br>

::: {.fragment fragment-index="2"}
```{r}
# confidence level: 90%
qt(0.95, df = nrow(football) - 2 - 1)
```
:::

<br>

::: {.fragment fragment-index="3"}
```{r}
# confidence level: 99%
qt(0.995, df = nrow(football) - 2 - 1)
```
:::

## 95% CI for $\beta_j$: Calculation

```{r}
#| echo: false
tidy(exp_fit) |> 
  kable(digits = 3)
```

## 95% CI for $\beta_j$ in R

```{r}
#| echo: true

tidy(exp_fit, conf.int = TRUE, conf.level = 0.95) |> 
  kable(digits = 3)
```

<br>

**Interpretation**: We are 95% confident that for each additional 1,000 students enrolled, the institution's expenditures on football will be greater by \$562,000 to \$999,000, on average, holding institution type constant.

# Test for overall significance

## Test for overall significance: Hypotheses

We can conduct a hypothesis test using the ANOVA table to determine if there is at least one non-zero coefficient in the model

$$
\begin{aligned}
&H_0: \beta_1 = \dots = \beta_p = 0\\
&H_a: \beta_j \neq 0 \text{ for at least one }j 
\end{aligned}
$$

. . .

**For the football data**

$$
\begin{aligned}
&H_0: \beta_1 = \beta_2 = 0\\
&H_a: \beta_j \neq 0 \text{ for at least one }j 
\end{aligned}
$$

## Test for overall significance: Test statistic {.midi}

```{r}
#| echo: false

football_anova <- anova(lm(total_exp_m ~ enrollment_th + type, data = football)) |>
  tidy() |>
  mutate(term = if_else(term %in% c("enrollment_th", "type"), "Model", term))

ssm <- round(football_anova$sumsq[1] + football_anova$sumsq[2], 3)
dfm <- football_anova$df[1] + football_anova$df[2]
msm <- ssm / dfm
ssr <- round(football_anova$sumsq[3], 3)
sst <- ssm + ssr
stat <- msm / football_anova$meansq[3]


#replace model row with total model values 
football_anova$df[1] = dfm
football_anova$sumsq[1] = ssm
football_anova$meansq[1] = msm
football_anova$statistic[1] = stat
football_anova$p.value[1] = pf(stat, dfm, football_anova$df[3], lower.tail = FALSE)


football_anova <- football_anova |>
  slice(-2) |> # remove extra row for model variable
  add_row(term = "Total", df = nrow(football) - 1, sumsq = sst, meansq = NA, statistic = NA, p.value = NA) 

football_anova <- football_anova |>
  mutate(meansq = round(meansq,3), 
         statistic = round(statistic, 3), 
         p.value = round(p.value, 3), 
         meansq = as.character(meansq), 
         statistic = as.character(statistic), 
         p.value = as.character(p.value),
         meansq = if_else(is.na(meansq), "", meansq), 
         statistic = if_else(is.na(statistic), "", statistic), 
         p.value = if_else(is.na(p.value), "", p.value))

football_anova <- football_anova |>
  rename("Source" = term, 
         "Df" = df,
         "Sum Sq" = sumsq, 
         "Mean Sq" = meansq, 
         "F Stat" = statistic, 
         "Pr(> F)" = p.value) 



football_anova |>
  kable(digits = 3)
```

<br>

**Test statistic**: Ratio of explained to unexplained variability

$$
F = \frac{\text{Mean Square Model}}{\text{Mean Square Residuals}}
$$

The test statistic follows an $F$ distribution with $p$ and $n -  p - 1$ degrees of freedom

## Test for overall significance: P-value

```{r}
#| echo: false

## F distribution 
ggplot() +
  stat_function(fun = df, 
                geom = "line", 
                args = list(df1 = 2, df2 = 124), 
                color = "steelblue2", lwd = 2, 
                xlim = c(0, 6)) +
  labs(x = "F", 
       y = "density", 
       title = "F distribution with 2 and 124 df") + 
  theme_classic()
```

$$
\text{P-value} = \text{Pr}(F > \text{F Stat})
$$

## Test for overall significance: Conclusion {.midi}

$$
\begin{aligned}
&H_0: \beta_1 = \beta_2 = 0\\
&H_a: \beta_j \neq 0 \text{ for at least one }j 
\end{aligned}
$$

```{r}
football_anova |>
  kable(digits = 3)
```

::: question
What is the conclusion from this hypothesis test?
:::

## Recap

-   Introduced statistical inference in the context of regression

-   Described the assumptions for regression

-   Connected the distribution of residuals and inferential procedures

-   Conducted inference on a single coefficient

-   Conducted inference on the overall regression model
