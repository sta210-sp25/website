---
title: "Multicollinearity + Variable transformations"
author: "Prof. Maria Tackett"
date: "2024-10-22"
date-format: "MMM DD, YYYY"
footer: "[🔗 STA 221 - Fall 2024](https://sta221-fa24.netlify.app)"
logo: "../images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: true
    incremental: false 
    chalkboard: true
    include-before: [ '<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
execute:
  freeze: auto
  echo: true
knitr:
  opts_chunk: 
    R.options:      
    width: 200
filters:
  - parse-latex
bibliography: references.bib
---

## Announcements

-   Exam corrections (optional) due Thursday at 11:59pm [on Canvas](https://canvas.duke.edu/courses/38867/assignments/169519)

-   Lab 04 due Thursday at 11:59pm

-   Team Feedback (from TEAMMATES) due Thursday at 11:59pm

-   [Mid semester survey](https://duke.qualtrics.com/jfe/form/SV_244HYi8U8X85pgW) (strongly encouraged!) by Thursday at 11:59pm

-   Looking ahead

    -   Project: Exploratory data analysis due October 31

    -   Statistics experience due Tuesday, November 26

## Spring 2025 statistics classes

-   STA 230, STA 231 or STA 240: Probability

-   STA 310: Generalized Linear Models

-   STA 323: Statistical Computing

-   STA 360: Bayesian Inference and Modern Statistical Methods

-   STA 432: Theory and Methods of Statistical Learning and Inference

## Computing set up

```{r}
#| echo: true
#| message: false

# load packages
library(tidyverse)  
library(tidymodels)  
library(knitr)       
library(patchwork)
library(GGally) #for pairwise plot matrix


# set default theme in ggplot2
ggplot2::theme_set(ggplot2::theme_bw())
```

## Topics

-   Multicollinearity

    -   Definition

    -   How it impacts the model

    -   How to detect it

    -   What to do about it

-   Variable transformations

## Data: Trail users {.midi}

-   The Pioneer Valley Planning Commission (PVPC) collected data at the beginning a trail in Florence, MA for ninety days from April 5, 2005 to November 15, 2005 to
-   Data collectors set up a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station.

```{r}
#| echo: false
#| message: false
#| warning: false

rail_trail <- read_csv(here::here("slides", "data/rail_trail.csv"))
rail_trail |> slice(1:5)
```

Source: [Pioneer Valley Planning Commission](http://www.fvgreenway.org/pdfs/Northampton-Bikepath-Volume-Counts%20_05_LTA.pdf) via the **mosaicData** package.

## Variables

**Outcome**:

-   `volume` estimated number of trail users that day (number of breaks recorded)

**Predictors**

-   `hightemp` daily high temperature (in degrees Fahrenheit)

-   `avgtemp` average of daily low and daily high temperature (in degrees Fahrenheit)

-   `season` one of “Fall”, “Spring”, or “Summer”

-   `precip` measure of precipitation (in inches)

## EDA: Relationship between predictors

We can create a pairwise plot matrix using the `ggpairs` function from the **GGally** R package

```{r}
#| eval: false

rail_trail |>
  select(hightemp, avgtemp, season, precip) |>
  ggpairs()
```

## EDA: Relationship between predictors {.midi}

```{r}
#| echo: false
#| fig-align: center

rail_trail |>
  select(hightemp, avgtemp, season, precip) |>
  ggpairs()
```

::: question
What might be a potential concern with a model that uses high temperature, average temperature, season, and precipitation to predict volume?
:::

# Multicollinearity

## Multicollinearity

-   Ideally there is no linear relationship (dependence) between the predictors

    -   This is generally not the case in practice but is often not a major issue

-   **Multicollinearity**: there are near-linear dependencies between predictors

## Common sources of multicollinearity

::: incremental
-   Dependencies that generally occur in the population

-   How the model is defined and the variables that are included

-   Sample comes from only a subspace of the region of predictors

-   There are more predictor variables than observations
:::

## Detecting multicollinearity

-   **Variance Inflation Factor (VIF)**: measure of multicollinearity in the regression model

$$
VIF_j = \frac{1}{1 - R^2_j}
$$

where $R^2_j$ is the proportion of variation in $x_j$ that is explained by a linear combination of all the other predictors

## Detecting multicollinearity

-   Common practice uses threshold $VIF > 10$ as indication of concerning multicollinearity

-   Variables with similar values of VIF are typically the ones correlated with each other

-   Use the `vif()` function in the **rms** R package to calculate VIF

## Effects of multicollinearity

::: incremental
-   Large variance $(\hat{\sigma}^2_{\epsilon}(\mathbf{X}^T\mathbf{X})^{-1})$ in the model coefficients

    -   Different combinations of coefficient estimates produce equally good model fits

-   Unreliable statistical inference results

    -   May conclude coefficients are not statistically significant when there is, in fact, a relationship between the predictors and response

-   Interpretation of coefficient is no longer "holding all other variables constant", since this would be impossible for correlated predictors
:::

# Application exercise

::: appex
📋 [sta221-fa24.netlify.app/ae/ae-05-multicollinearity](../ae/ae-05-multicollinearity.html)
:::

Selected groups - put responses on your [Google slide](https://docs.google.com/presentation/d/1pFjNKtvyMo7rd0RJFVNvJf6UcRVsFnXeW4qWOMNeTdU/edit?usp=sharing).

## Dealing with multicollinearity

::: incremental
-   Collect more data (often not feasible given practical constraints)

-   Redefine the correlated predictors to keep the information from predictors but eliminate collinearity

    -   e.g., if $x_1, x_2, x_3$ are correlated, use a new variable $(x_1 + x_2) / x_3$ in the model

-   For categorical predictors, avoid using levels with very few observations as the baseline

-   Remove one of the correlated variables

    -   Be careful about substantially reducing predictive power of the model
:::

# Application exercise

::: appex
📋 [sta221-fa24.netlify.app/ae/ae-05-multicollinearity](../ae/ae-05-multicollinearity.html)
:::

Selected groups - put responses on your [Google slide](https://docs.google.com/presentation/d/1pFjNKtvyMo7rd0RJFVNvJf6UcRVsFnXeW4qWOMNeTdU/edit?usp=sharing).

# Variable transformations

## Data: Respiratory Rate vs. Age {.midi}

-   A high respiratory rate can potentially indicate a respiratory infection in children. In order to determine what indicates a "high" rate, we first want to understand the relationship between a child's age and their respiratory rate.

-   The data contain the respiratory rate for 618 children ages 15 days to 3 years. It was obtained from the **Sleuth3** R package and is originally form a 1994 publication "Reference Values for Respiratory Rate in the First 3 Years of Life".

-   **Variables**:

    -   `Age`: age in months
    -   `Rate`: respiratory rate (breaths per minute)

## Rate vs. Age

```{r}
#| echo: false

respiratory <- Sleuth3::ex0824 |>
  mutate(log_rate = log(Rate), 
         log_age = log(Age))
ggplot(data=respiratory, aes(x=Age, y=Rate)) +
  geom_point(alpha = 0.7) + 
  geom_smooth() +
  labs(title  = "Respiratory Rate vs. Age", 
       x = "Age in months", 
       y = "Respiratory rate in breaths per minute")
```

## Model 1: Rate vs. Age

```{r}
resp_fit <- lm(Rate ~ Age, data = respiratory)

tidy(resp_fit) |>
  kable(digits = 3)
```

## Model 1: Residuals

```{r echo=FALSE}
resp_aug <- augment(resp_fit)

resid_orig <- ggplot(data= resp_aug, aes(x=.fitted, y=.resid)) +
  geom_point(alpha = 0.7) + 
  geom_hline(yintercept=0,color="red") + 
  labs(x = "Predicted", y = "Residuals", 
       title = "Model 1: Residuals vs. Predicted")

resid_orig
```

## Consider different transformations...

```{r}
#| echo: false

p1 <- ggplot(data=respiratory, aes(x=Age, y=Rate)) +
  geom_point(alpha = 0.7) + 
  geom_smooth(method = "lm")
p2 <- ggplot(data=respiratory, aes(x=Age, y=log_rate)) +
  geom_point(alpha = 0.7) + 
  geom_smooth(method = "lm") + 
    labs(y = "log(Rate)")
p3 <- ggplot(data=respiratory, aes(x=log_age, y=Rate)) +
  geom_point(alpha = 0.7) + 
  geom_smooth(method = "lm") + 
    labs(x = "log(Age)")
  
p4 <- ggplot(data=respiratory, aes(x=log_age, y=log_rate)) +
  geom_point(alpha = 0.7) + 
  geom_smooth(method = "lm") + 
    labs(x = "log(Age)", y = "log(Rate)")
  
(p1 + p2) / (p3 + p4)
```

# Transformation on $Y$

## Identifying a need to transform Y

::: incremental
-   Typically, a “fan-shaped” residual plot indicates the need for a transformation of the response variable Y

    -   There are multiple ways to transform a variable, e.g., Y, 1/Y, log⁡(Y)

    -   log⁡(Y) the most straightforward to interpret, so we use that transformation when possible

<!-- -->

-   When building a model:

    -   Choose a transformation and build the model on the transformed data

    -   Reassess the residual plots

    -   If the residuals plots did not sufficiently improve, try a new transformation!
:::

## Log transformation on $Y$

-   If we apply a log transformation to the response variable, we want to estimate the parameters for the statistical model

$$
\log(y_i) = \beta_0+ \beta_1 x_{i1} + \dots +\beta_px_{ip} + \epsilon_i, \hspace{10mm} \epsilon \sim N(0,\sigma^2_\epsilon)
$$

-   The regression equation is

$$\widehat{\log(y_i)} = \hat{\beta}_0+ \hat{\beta}_1 x_{i1} + \dots + \hat{\beta}_px_{ip}$$

## Log transformation on $Y$

We want to interpret the model in terms of the original variable $Y$, not $\log(Y)$, so we need to write the regression equation in terms of $Y$

$$\begin{align}\hat{y_i} &= \exp\{\hat{\beta}_0 + \hat{\beta}_1 x_{i1} + \dots + \hat{\beta}_Px_{ip}\}\\ &= \exp\{\hat{\beta}_0\}\exp\{\hat{\beta}_1x_{i1}\}\dots\exp\{\hat{\beta}_px_{ip}\}\end{align}$$

::: callout-note
The predicted value $\hat{y_i}$ is the predicted *median* of $Y$. Note, when the distribution of $y_i|x_1, \ldots, x_p$ is symmetric, then the median equals the mean. (See notes at the end for more details)
:::

## Model interpretation {.midi}

$$\begin{align}\hat{y_i} &= \exp\{\hat{\beta}_0 + \hat{\beta}_1 x_{1p} + \dots + \hat{\beta}_Px_{ip}\}\\ &= \exp\{\hat{\beta}_0\}\exp\{\hat{\beta}_1x_{i1}\}\dots\exp\{\hat{\beta}_px_{ip}\}\end{align}$$

. . .

-   **Intercept**: When $x_{i1} = \dots = x_{ip} =0$, $y_i$ is expected to be $\exp\{\hat{\beta}_0\}$

-   **Slope:** For every one unit increase in $x_{ij}$, $y_{i}$ is expected to multiply by a factor of $\exp\{\hat{\beta}_j\}$, holding all else constant

::: question
Why is the interpretation in terms of a multiplicative change?
:::

## Model 2: log(Rate) vs. Age

```{r}
#| echo: false
#fit model
resp_logy_fit <- lm(log_rate ~ Age, data = respiratory)

tidy(resp_logy_fit) |>
  kable(digits = 3)
```

<br>

::: question
-   Interpret the intercept in terms of (1) `log(Rate)` and (2) `Rate`.

-   Interpret the effect of `Age` in terms of (1) `log(Rate)` and (2) `Rate`.
:::

## Model 2: Residuals

```{r echo=F}
resp_logy_aug <- augment(resp_logy_fit)

resid_logy <- ggplot(data = resp_logy_aug, aes(x = .fitted, y = .resid)) +
  geom_point(alpha = 0.7) + 
  geom_hline(yintercept=0, color="red") +
  labs(x="Predicted", y="Residuals",
       title="Model 2: Residuals vs. Predicted")

resid_logy
```

## Compare residual plots

```{r}
#| echo: false

resid_orig + resid_logy
```

# Log transformation on a predictor variable

## Log Transformation on $X$

```{r,echo=F}
set.seed(1)
s <- ggplot2::diamonds |> sample_n(100)
p1 <- ggplot(data=s,aes(x=carat,y=log(price)))+
  geom_point(color="blue")+
  ggtitle("Scatterplot")+
  xlab("X")+
  ylab("Y")
```

```{r,echo=F}
mod2 <- lm(log(price) ~ carat, data=s)
s <- s |> mutate(residuals = resid(mod2), predicted = predict(mod2))
p2 <- ggplot(data=s,aes(x=predicted, y=residuals)) + 
geom_point(alpha = 0.7)+
geom_hline(yintercept=0,color="red") +
  ggtitle("Residual vs. Predicted")+
  xlab("Predicted")+
  ylab("residuals") 
```

```{r, echo = F}
p1 + p2 + plot_annotation(title = "Example data")
```

Try a transformation on $X$ if the scatterplot shows some curvature but the variance is constant for all values of $X$

## Rate vs. log(Age)

```{r,echo=F}
ggplot(data= respiratory,aes(x=log_age,y=Rate)) + 
  geom_point(alpha = 0.7)  +
  ggtitle("Respiratory Rate vs. log(Age)") + 
  xlab("log(Age)")+
  ylab("Respiratory Rate")
```

## Model with Transformation on $X$ {.midi}

Suppose we have the following regression equation:

$$\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 \log(x_i)$$

. . .

::: incremental
-   **Intercept:** When $x_i = 1$ $(\log(x_i) = 0)$, $y_i$ is expected to be $\hat{\beta}_0$ (i.e. the mean of $y_i$ is $\hat{\beta}_0$)

-   **Slope:** When $x_i$ is multiplied by a factor of $\mathbf{C}$, the mean of $y_i$ is expected to increase by $\hat{\beta}_1\log(C)$ units

    -   **Example**: when $x_i$ is multiplied by a factor of 2, $y_i$ is expected to increase by $\hat{\beta}_1}\mathbf{\log(2)$ units
:::

## Model 3: Rate vs. log(Age)

```{r,echo=F}
resp_logx_fit <- lm(Rate ~ log_age, data = respiratory)

tidy(resp_logx_fit) |>
  kable(digits = 3)
```

<br>

::: question
Interpret the slope and intercept in the context of the data.
:::

## Model 3: Residuals

```{r}
#| echo: false
resp_logx_aug <- augment(resp_logx_fit)

resid_logx <- ggplot(data = resp_logx_aug, aes(x = .fitted, y = .resid)) +
  geom_point(alpha = 0.7) + 
  geom_hline(yintercept=0, color="red") +
  labs(x="Predicted", y="Residuals",
       title="Model 3: Residuals vs. Predicted")

resid_logx
```

## Choose a model

Recall the goal of the analysis:

*In order to determine what indicates a "high" rate, we first want to understand the relationship between a child's age and their respiratory rate.*

<br>

::: question
Which is the preferred metric to compare the models - $R^2$ or RMSE?
:::

## Compare models

```{r}
#| echo: false

m1_aug <- augment(resp_fit)

m2_aug <- augment(resp_logy_fit) 

m3_aug <- augment(resp_logx_fit)
```

| Rate vs. Age                            | log(Rate) vs. Age                            | Rate vs. log(Age)                            |
|------------------------|-------------------------|------------------------|
| `r round(glance(resp_fit)$r.squared,3)` | `r round(glance(resp_logy_fit)$r.squared,3)` | `r round(glance(resp_logx_fit)$r.squared,3)` |

<br>

::: question
Which model would you choose?
:::

## Learn more

See [Log Transformations in Linear Regression](https://github.com/sta210-sp20/supplemental-notes/blob/master/log-transformations.pdf) for more details about interpreting regression models with log-transformed variables.

## Recap

-   Introduced multicollinearity

    -   Definition

    -   How it impacts the model

    -   How to detect it

    -   What to do about it

-   Introduced variable transformations
