---
title: "Multicollinearity + Variable transformations"
author: "Prof. Maria Tackett"
date: "2024-10-22"
date-format: "MMM DD, YYYY"
footer: "[🔗 STA 221 - Fall 2024](https://sta221-fa24.netlify.app)"
logo: "../images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: true
    incremental: false 
    chalkboard: true
    include-before: [ '<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
execute:
  freeze: auto
  echo: true
knitr:
  opts_chunk: 
    R.options:      
    width: 200
filters:
  - parse-latex
bibliography: references.bib
---

## Announcements

-   Exam corrections (optional) due Thursday at 11:59pm [on Canvas](https://canvas.duke.edu/courses/38867/assignments/169519)

-   Lab 04 due Thursday at 11:59pm

-   Looking ahead

    -   Project: Exploratory data analysis due October 31

    -   Statistics experience due Tuesday, November 26

## Spring 2025 statistics classes

## Computing set up

```{r}
#| echo: true
#| message: false

# load packages
library(tidyverse)  
library(tidymodels)  
library(knitr)       
library(patchwork)
library(GGally) #for scatterplot matrix


# set default theme in ggplot2
ggplot2::theme_set(ggplot2::theme_bw())
```

## Topics

-   Multicollinearity

    -   Definition

    -   How it impacts the model

    -   How to detect it

    -   What to do about it

-   Variable transformations

## Data: `rail_trail` {.midi}

-   The Pioneer Valley Planning Commission (PVPC) collected data for ninety days from April 5, 2005 to November 15, 2005.
-   Data collectors set up a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station.

```{r}
#| echo: false
#| message: false
#| warning: false

rail_trail <- read_csv(here::here("slides", "data/rail_trail.csv"))
rail_trail
```

Source: [Pioneer Valley Planning Commission](http://www.fvgreenway.org/pdfs/Northampton-Bikepath-Volume-Counts%20_05_LTA.pdf) via the **mosaicData** package.

## Variables

**Outcome**:

`volume` estimated number of trail users that day (number of breaks recorded)

**Predictors**

-   `hightemp` daily high temperature (in degrees Fahrenheit)

-   `avgtemp` average of daily low and daily high temperature (in degrees Fahrenheit)

-   `season` one of “Fall”, “Spring”, or “Summer”

-   `precip` measure of precipitation (in inches)

## EDA: Relationship between predictors

We can create a scatterplot matrix using the `ggpairs` function from the **GGally** R package

```{r}
#| eval: false

rail_trail |>
  select(hightemp, avgtemp, season, precip) |>
  ggpairs()
```

## EDA: Relationship between predictors {.midi}

```{r}
#| echo: false
#| fig-align: center

rail_trail |>
  select(hightemp, avgtemp, season, precip) |>
  ggpairs()
```

::: question
What looks like a potential concern with a model that uses high temperature, average temperature, season, and precipitation to predict volume?
:::

# Multicollinearity

## Multicollinearity

-   Predictors are orthogonal when there is no linear relationship (dependence) between the predictors

    -   This is generally not the case in practice but is often not a major issue

-   **Multicollinearity**: there are near-linear dependencies between predictors

## Common causes of multicollinearity

-   Dependencies that generally occur in the population

-   How the model is defined and the variables that are included

-   Sample comes from only a subspace of the region of predictors

-   There are more predictor variables than observations

## Detecting multicollinearity

-   **Variance Inflation Factor (VIF)**: Measure of multicollinearity in the regression model

$$
VIF_j = \frac{1}{1 - R^2_j}
$$

where $R^2_j$ is the proportion of variation in $x_j$ that is explained by a linear combination of all the other predictors

## Effects of multicollinearity

-   Large variance $(\hat{\sigma}^2_{\epsilon}(\mathbf{X}^T\mathbf{X})^{-1})$ in the model coefficients

    -   Different combinations of coefficient estimates produce equally good model fits

-   Unreliable statistical inference results

    -   May conclude coefficients are not statistically significant when there is, in fact, a relationship

-   Interpretation of coefficient is no longer while holding all other variables constant, since this would be impossible for correlated predictors

## Detecting multicollinearity

-   Common practice uses threshold $VIF > 10$ as indication of concerning multicollinearity

-   Variables with similar values of VIF are typically the ones correlated with each other

-   Use the `vif()` function in the **rms** R package to calculate VIF

# Application exercise

## Dealing with multicollinearity

-   Collect more data

    -   Often not feasible given practical constraints

-   Respecify the correlated predictors to keep the information from predictors but eliminate collinearity

    -   e.g., if $x_1, x_2, x_3$ are correlated, use a new variable $(x_1 + x_2) / x_3$ in the model

-   For categorical predictors, avoid using levels with very few observations as the baseline

-   Remove one of the correlated variables

    -   Be careful about substantially reducing predictive power of the model

# Application exercise

# Variable transformations

## Respiratory Rate vs. Age {.midi}

-   A high respiratory rate can potentially indicate a respiratory infection in children. In order to determine what indicates a "high" rate, we first want to understand the relationship between a child's age and their respiratory rate.

-   The data contain the respiratory rate for 618 children ages 15 days to 3 years. It was obtained from the **Sleuth3** R package and is originally form a 1994 publication "Reference Values for Respiratory Rate in the First 3 Years of Life".

-   **Variables**:

    -   `Age`: age in months
    -   `Rate`: respiratory rate (breaths per minute)

## Rate vs. Age

```{r}
#| echo: false

respiratory <- Sleuth3::ex0824 |>
  mutate(log_rate = log(Rate), 
         log_age = log(Age))
ggplot(data=respiratory, aes(x=Age, y=Rate)) +
  geom_point(alpha = 0.7) + 
  geom_smooth() +
  labs(title  = "Respiratory Rate vs. Age", 
       x = "Age in months", 
       y = "Respiratory rate in breaths per minute")
```

## Model 1: Rate vs. Age

```{r}
resp_fit <- lm(Rate ~ Age, data = respiratory)

tidy(resp_fit) |>
  kable(digits = 3)
```

## Model 1: Residuals

```{r echo=FALSE}
resp_aug <- augment(resp_fit)

resid_orig <- ggplot(data= resp_aug, aes(x=.fitted, y=.resid)) +
  geom_point(alpha = 0.7) + 
  geom_hline(yintercept=0,color="red") + 
  labs(x = "Predicted", y = "Residuals", 
       title = "Model 1: Residuals vs. Predicted")

resid_orig
```

## Consider different transformations...

```{r}
#| echo: false

p1 <- ggplot(data=respiratory, aes(x=Age, y=Rate)) +
  geom_point(alpha = 0.7) + 
  geom_smooth(method = "lm")
p2 <- ggplot(data=respiratory, aes(x=Age, y=log_rate)) +
  geom_point(alpha = 0.7) + 
  geom_smooth(method = "lm") + 
    labs(y = "log(Rate)")
p3 <- ggplot(data=respiratory, aes(x=log_age, y=Rate)) +
  geom_point(alpha = 0.7) + 
  geom_smooth(method = "lm") + 
    labs(x = "log(Age)")
  
p4 <- ggplot(data=respiratory, aes(x=log_age, y=log_rate)) +
  geom_point(alpha = 0.7) + 
  geom_smooth(method = "lm") + 
    labs(x = "log(Age)", y = "log(Rate)")
  
(p1 + p2) / (p3 + p4)
```

## Model 2: log(Rate) vs. Age

```{r}
#| echo: false
#fit model
resp_logy_fit <- lm(log_rate ~ Age, data = respiratory)

tidy(resp_logy_fit) |>
  kable(digits = 3)
```

<br>

-   **Slope:** For each additional month in a child's age, the median respiratory rate is expected to multiply by a factor of `r round(exp(-0.018), 3)` \[exp(-0.018)\].

-   **Intercept:** The median respiratory rate for children who are 0 months old is expected to be `r round(exp(3.381), 3)` \[exp(3.381)\].

## Model 2: Residuals

```{r echo=F}
resp_logy_aug <- augment(resp_logy_fit)

resid_logy <- ggplot(data = resp_logy_aug, aes(x = .fitted, y = .resid)) +
  geom_point(alpha = 0.7) + 
  geom_hline(yintercept=0, color="red") +
  labs(x="Predicted", y="Residuals",
       title="Model 2: Residuals vs. Predicted")

resid_logy
```

## Compare residual plots

```{r}
#| echo: false

resid_orig + resid_logy
```

# Log transformation on a predictor variable

## Log Transformation on $X$

```{r,echo=F}
set.seed(1)
s <- ggplot2::diamonds |> sample_n(100)
p1 <- ggplot(data=s,aes(x=carat,y=log(price)))+
  geom_point(color="blue")+
  ggtitle("Scatterplot")+
  xlab("X")+
  ylab("Y")
```

```{r,echo=F}
mod2 <- lm(log(price) ~ carat, data=s)
s <- s |> mutate(residuals = resid(mod2), predicted = predict(mod2))
p2 <- ggplot(data=s,aes(x=predicted, y=residuals)) + 
geom_point(alpha = 0.7)+
geom_hline(yintercept=0,color="red") +
  ggtitle("Residual vs. Predicted")+
  xlab("Predicted")+
  ylab("residuals") 
```

```{r, echo = F, fig.height = 2.5}
p1 + p2 + plot_annotation(title = "Example data")
```

Try a transformation on $X$ if the scatterplot shows some curvature but the variance is constant for all values of $X$

## Rate vs. log(Age)

```{r,echo=F}
ggplot(data= respiratory,aes(x=log_age,y=Rate)) + 
  geom_point(alpha = 0.7)  +
  ggtitle("Respiratory Rate vs. log(Age)") + 
  xlab("log(Age)")+
  ylab("Respiratory Rate")
```

## Model with Transformation on $X$ {.midi}

Suppose we have the following regression equation:

$$\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 \log(X)$$

. . .

-   **Intercept:** When $X = 1$ $(\log(X) = 0)$, $Y$ is expected to be $\hat{\beta}_0$ (i.e. the mean of $Y$ is $\hat{\beta}_0$)

-   **Slope:** When $X$ is multiplied by a factor of $\mathbf{C}$, the mean of $Y$ is expected to increase by $\boldsymbol{\hat{\beta}_1}\mathbf{\log(C)}$ units

    -   **Example**: when $X$ is multiplied by a factor of 2, $Y$ is expected to increase by $\boldsymbol{\hat{\beta}_1}\mathbf{\log(2)}$ units

## Model 3: Rate vs. log(Age)

```{r,echo=F}
resp_logx_fit <- lm(Rate ~ log_age, data = respiratory)

tidy(resp_logx_fit) |>
  kable(digits = 3)
```

<br>

::: question
Interpret the slope and intercept in the context of the data.
:::

```{r}
#| echo: false
countdown::countdown(minutes = 4)
```

## Model 3: Residuals

```{r}
#| echo: false
resp_logx_aug <- augment(resp_logx_fit)

resid_logx <- ggplot(data = resp_logx_aug, aes(x = .fitted, y = .resid)) +
  geom_point(alpha = 0.7) + 
  geom_hline(yintercept=0, color="red") +
  labs(x="Predicted", y="Residuals",
       title="Model 3: Residuals vs. Predicted")

resid_logx
```

## Choose a model

Recall the goal of the analysis:

*In order to determine what indicates a "high" rate, we first want to understand the relationship between a child's age and their respiratory rate.*

<br>

::: question
Which is the preferred metric to compare the models - $R^2$ or RMSE?
:::

## Compare models

```{r}
#| echo: false

m1_aug <- augment(resp_fit)

m2_aug <- augment(resp_logy_fit) 

m3_aug <- augment(resp_logx_fit)
```

| Rate vs. Age                            | log(Rate) vs. Age                            | Rate vs. log(Age)                            |
|------------------------|-------------------------|------------------------|
| `r round(glance(resp_fit)$r.squared,3)` | `r round(glance(resp_logy_fit)$r.squared,3)` | `r round(glance(resp_logx_fit)$r.squared,3)` |

<br>

::: question
Which model would you choose?
:::

## Learn more

See [Log Transformations in Linear Regression](https://github.com/sta210-sp20/supplemental-notes/blob/master/log-transformations.pdf) for more details about interpreting regression models with log-transformed variables.

## Recap

-   Introduced multicollinearity

    -   Definition

    -   How it impacts the model

    -   How to detect it

    -   What to do about it

-   Introduced variable transformations
