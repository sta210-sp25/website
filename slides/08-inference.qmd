---
title: "Inference for regression"
author: "Prof. Maria Tackett"
date: "2024-09-19"
date-format: "MMM DD, YYYY"
footer: "[ðŸ”— STA 221 - Fall 2024](https://sta221-fa24.netlify.app)"
logo: "../images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: true
    incremental: false 
    chalkboard: true
    include-before: [ '<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
execute:
  freeze: auto
  echo: true
knitr:
  opts_chunk: 
    R.options:      
    width: 200
filters:
  - parse-latex
bibliography: references.bib
---

```{r setup}
#| include: false

library(countdown)

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = 0.618,
  fig.retina = 3,
  dpi = 300,
  out.width = "80%",
  fig.align = "center"
)

options(scipen=999)
```

## Announcements

-   Lab 02 due on **TODAY at 11:59pm**

-   HW 01 due **TODAY at 11:59pm**

-   [Statistics experience](https://sta221-fa24.netlify.app/hw/stats-experience) due **Tue, Nov 26 at 11:59pm**

## Statistics experience

**Goal:** Engage with statistics / data science outside the classroom and connect your experience with what youâ€™re learning in the course.

**What:** Have a statistics experience + create a slide reflecting on the experience. Counts as a homework grade.

**When:** Must do the activity this semester. Reflection due **Tuesday, November 26 at 11:59pm**

For more info: [sta221-fa24.netlify.app/hw/stats-experience](https://sta221-fa24.netlify.app/hw/stats-experience)

## Reminder: course policies about assignments

-   [Late work](https://sta221-fa24.netlify.app/syllabus#late-work-policy)

    -   HW and labs accepted up to 2 days late.
    -   5% deduction for each 24-hour period the assignment is late.

-   [One time late waiver](https://sta221-fa24.netlify.app/syllabus#waiver-for-extenuating-circumstances)

    -   Can use on HW and individual labs

-   Lowest HW and lowest lab grade dropped at the end of the semester.

## Reminder: course policies about assignments {.midi}

-   **Read the feedback on Gradescope carefully!** If you have questions about the comments, ask a member of the teaching team during office hours or before/after class.
-   [Regrade requests](https://sta221-fa24.netlify.app/syllabus#regrade-requests)
    -   Opened 1 day after assignment is returned and due within 1 week
    -   Only submit regrade request if there is an error in the grading not to dispute points or ask questions about grading.
    -   Prof. Tackett or Kat (Head TA) will regrade the entire exercise being disputed, which could potentially result in a lower grade.

## Poll: Office hours availability

```{=html}
<iframe width="640px" height="480px" src="https://forms.office.com/Pages/ResponsePage.aspx?id=TsVyyzFKnk2xSh6jbfrJTBw0r2_bKCVMs9lST1_-2sxUNURRQzlXME43RUJSVk9KT1pSVlpBRlZMSi4u&embed=true" frameborder="0" marginwidth="0" marginheight="0" style="border: none; max-width:100%; max-height:100vh" allowfullscreen webkitallowfullscreen mozallowfullscreen msallowfullscreen> </iframe>
```
## Topics

-   Understand statistical inference in the context of regression

-   Describe the assumptions for regression

-   Understand connection between distribution of residuals and inferential procedures

-   Conduct inference on a single coefficient

-   Conduct inference on the overall regression model

## Computing setup

```{r packages}
#| echo: true
#| message: false

# load packages
library(tidyverse)  
library(tidymodels)  
library(knitr)       
library(kableExtra)  
library(patchwork)   

# set default theme in ggplot2
ggplot2::theme_set(ggplot2::theme_bw())
```

## Data: NCAA Football expenditures {.midi}

Today's data come from [Equity in Athletics Data Analysis](https://ope.ed.gov/athletics/#/datafile/list) and includes information about sports expenditures and revenues for colleges and universities in the United States. This data set was featured in a [March 2022 Tidy Tuesday](https://github.com/rfordatascience/tidytuesday/blob/master/data/2022/2022-03-29/readme.md).

We will focus on the 2019 - 2020 season expenditures on football for institutions in the NCAA - Division 1 FBS. The variables are :

-   `total_exp_m`: Total expenditures on football in the 2019 - 2020 academic year (in millions USD)

-   `enrollment_th`: Total student enrollment in the 2019 - 2020 academic year (in thousands)

-   `type`: institution type (Public or Private)

```{r}
#| include: false
#| eval: false

## code to make data set for these notes

sports <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-03-29/sports.csv') 

# filter data to only include D1 football for the year 2019

sports |>
  filter(sports == "Football", 
         classification_name == "NCAA Division I-FBS", year == 2019) |>
  mutate(type = if_else(sector_name == "Private nonprofit, 4-year or above", "Private", "Public"), 
         enrollment_th = ef_total_count / 1000,
         total_exp_m = total_exp_menwomen/ 1000000) |>
  select(year, institution_name, city_txt, state_cd, zip_text, type,
         enrollment_th, 
         total_exp_m) |> 
  write_csv("data/ncaa-football-exp.csv")


```

```{r}
football <- read_csv("data/ncaa-football-exp.csv")
```

## Univariate EDA

```{r}
#| echo: false

p1 <- ggplot(data = football, aes(x = total_exp_m)) + 
  geom_histogram(fill = "steelblue", color = "black", binwidth = 5) + 
  labs( x= "Total Football Expenditures (in $Millions)")

p2 <- ggplot(data = football, aes(x = enrollment_th)) + 
  geom_histogram(binwidth = 5, fill = "steelblue", color = "black") +
  labs(x = "Total Student Enrollment (in Thousands)")

p3 <- ggplot(data = football, aes(x = type)) + 
  geom_bar(fill = "steelblue", color = "black") + 
  labs(x = "Insitution Type")

p1 + (p2 / p3)
```

## Bivariate EDA

```{r}
#| echo: false
#| 
p4 <- ggplot(data = football, aes(x = enrollment_th, y = total_exp_m)) +
  geom_point() +
  labs(x = "Total Student Enrollment (in Thousands)", 
       y = "Total Football Expenditures (in $Millions)", 
       title = "Football Expenditures vs. Enrollment")

p5 <- ggplot(data = football, aes(x = type, y = total_exp_m, fill = type)) + 
  geom_boxplot() +
  labs(x = "Institution Type", 
       y = "",
       title = "Football Expenditure vs. Type") + 
  theme(legend.position = "none")

p4 + p5
```

## Regression model

```{r}
#| echo: true
exp_fit <- lm(total_exp_m ~ enrollment_th + type, data = football)
tidy(exp_fit) |>
  kable(digits = 3)
```

<br>

For every additional 1,000 students, we expect the institution's total expenditures on football to increase by \$780,000, on average, holding institution type constant.

## From sample to population {.midi}

> For every additional 1,000 students, we expect the institution's total expenditures on football to increase by \$780,000, on average, holding institution type constant.

. . .

<br>

-   This estimate is valid for the single sample of `r nrow(football)` higher education institutions in the 2019 - 2020 academic year.
-   But what if we're not interested quantifying the relationship between student enrollment, institution type, and football expenditures for this single sample?
-   What if we want to say something about the relationship between these variables for all colleges and universities with football programs and across different years?

# Inference for regression

## Statistical inference

::: columns
::: {.column width="40%"}
-   **Statistical inference** provides methods and tools so we can use the single observed sample to make valid statements (inferences) about the population it comes from

-   For our inferences to be valid, the sample should be representative (ideally random) of the population we're interested in
:::

::: {.column width="60%"}
![Image source: Eugene Morgan Â© Penn State](images/08/inference.png){fig-align="center"}
:::
:::

## Inference for linear regression

-   **Inference based on ANOVA**

    -   Hypothesis test for the statistical significance of the overall regression model

    -   Hypothesis test for a subset of coefficients

-   **Inference for a single coefficient** $\beta_j$

    -   Hypothesis test for a coefficient $\beta_j$

    -   Confidence interval for a coefficient $\beta_j$

## Linear regression model {.midi}

$$
\begin{aligned}
\mathbf{y} &= Model + Error \\[5pt]
&= f(\mathbf{X}) + \boldsymbol{\epsilon} \\[5pt]
&= E(\mathbf{y}|\mathbf{X}) + \mathbf{\epsilon} \\[5pt]
&= \mathbf{X}\boldsymbol{\beta} + \mathbf{\epsilon}
\end{aligned}
$$

. . .

::: incremental
-   We have discussed multiple ways to find the least squares estimates of $\boldsymbol{\beta} = \begin{bmatrix}\beta_0 \\\beta_1\end{bmatrix}$

    -   None of these approaches depend on the distribution of $\boldsymbol{\epsilon}$

-   Now we will use statistical inference to draw conclusions about $\boldsymbol{\beta}$ that depend on particular assumptions about the distribution of $\boldsymbol{\epsilon}$
:::

## Linear regression model

$$\begin{aligned}
\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}, \hspace{8mm} \boldsymbol{\epsilon} \sim N(0, \sigma^2_{\epsilon}\mathbf{I})
\end{aligned}
$$

such that the errors are independent and normally distributed.

. . .

-   **Independent**: Knowing the error term for one observation doesn't tell you anything about the error term for another observation
-   **Normally distributed**: Tell us the shape of the distribution of residuals

::: question
What else do we know about the distribution of the residuals based on this equation?
:::

## Describing random phenomena

::: incremental
-   There is some uncertainty in the residuals (and the predicted responses), so we use mathematical models to describe that uncertainty.

-   Some terminology:

    -   **Sample space**: Set of all possible outcomes

    -   **Random variable**: Function (mapping) from the sample space onto real numbers

    -   **Event:** Subset of the sample space, i.e., a set of possible outcomes (possible values the random variable can take)

    -   **Probability distribution function:** Mathematical function that produces probability of occurrences for events in the sample space
:::

## Example {.midi}

Suppose we are tossing 2 fair coins with sides heads (H) and tails (T)

::: incremental
-   **Sample space**: {HH, HT, TH, TT}

-   **Random variable**: $X$ : The number of heads in two coin tosses

-   **Event:** We flip two coins and get 1 head

-   **Probability distribution function:** $$P(X = x_i) = {2 \choose x_i}0.5^{x_i}{0.5}^{2-x_i}$$

-   Now we can find $$P(X = 1) = {2 \choose 1}0.5^1{0.5}^{2-1} = 0.5$$
:::

## Mathematical representation {.midi}

$$
\mathbf{y}|\mathbf{X} \sim N(\mathbf{X}\boldsymbol{\beta}, \sigma_\epsilon^2\mathbf{I})
$$

![Image source: *Introduction to the Practice of Statistics (5th ed)*](images/08/regression.png){fig-align="center"}

## Expected value of $\mathbf{y}$ 

Let $\mathbf{b} = \begin{bmatrix}b_1 \\ \vdots \\b_p\end{bmatrix}$ be a $p \times 1$ vector of random variables.

<br>

. . .

Then $E(\mathbf{b}) = E\begin{bmatrix}b_1 \\ \vdots \\ b_p\end{bmatrix} = \begin{bmatrix}E(b_1) \\ \vdots \\ E(b_p)\end{bmatrix}$

<br>

. . .

::: question
Use this to find $E(\mathbf{y}|\mathbf{X})$.
:::

## Variance {.midi}

Let $\mathbf{b} = \begin{bmatrix}b_1 \\ \vdots \\b_p\end{bmatrix}$ be a $p \times 1$ vector of *independent* random variables.

<br>

. . .

Then $Var(\mathbf{b}) = \begin{bmatrix}Var(b_1) & 0 & \dots & 0 \\ 0 & Var(b_2) & \dots & 0 \\ \vdots & \vdots & \dots & \cdot \\ 0 & 0 & \dots & Var(b_p)\end{bmatrix}$

<br>

. . .

::: question
Use this to find $Var(\mathbf{y}|\mathbf{X})$.
:::

## Assumptions of regression {.midi}

::: columns
::: {.column width="50%"}
$$
\mathbf{y}|\mathbf{X} \sim N(\mathbf{X}\boldsymbol{\beta}, \sigma_\epsilon^2\mathbf{I})
$$

![Image source: *Introduction to the Practice of Statistics (5th ed)*](images/08/regression.png){fig-align="center"}
:::

::: {.column width="50%"}
1.  **Linearity:** There is a linear relationship between the response and predictor variables.
2.  **Constant Variance:** The variability about the least squares line is generally constant.
3.  **Normality:** The distribution of the residuals is approximately normal.
4.  **Independence:** The residuals are independent from one another.
:::
:::

## Estimating $\sigma^2_{\epsilon}$ {.midi}

-   Once we fit the model, we can use the residuals to estimate $\sigma_{\epsilon}^2$

-   $\hat{\sigma}^2_{\epsilon}$ is needed for hypothesis testing and constructing confidence intervals for regression

$$
\hat{\sigma}^2_\epsilon = \frac{\sum_\limits{i=1}^n(y_i - \hat{y}_i)^2}{n-p-1} = \frac{\sum_\limits{i=1}^ne_i^2}{n - p - 1} = \frac{SSR}{n - p - 1}
$$

. . .

-   The **regression standard error** $\hat{\sigma}_{\epsilon}$ is a measure of the average distance between the observations and regression line

$$
\hat{\sigma}_\epsilon = \sqrt{\frac{SSR}{n - p - 1}} 
$$

# Inference for a single coefficient

## Inference for $\beta_j$

We often want to conduct inference on individual model coefficients

-   **Hypothesis test:** Is there a linear relationship between the response and $x_j$?

-   **Confidence interval**: What is a plausible range of values $\beta_j$ can take?

. . .

But first we need to understand the distribution of $\hat{\beta}_j$

## Sampling distribution of $\hat{\beta}$ {.midi}

-   A **sampling distribution** is the probability distribution of a statistic based on a large number of random samples of size $n$ from a population

-   The sampling distribution of $\hat{\boldsymbol{\beta}}$ is the probability distribution of the estimated coefficients if we repeatedly took samples of size $n$ and fit the regression model

$$
\hat{\boldsymbol{\beta}} \sim N(\boldsymbol{\beta}, \sigma^2_\epsilon(\mathbf{X}^T\mathbf{X})^{-1})
$$

. . .

The estimated coefficients $\hat{\boldsymbol{\beta}}$ are **normally distributed** with

$$
E(\hat{\boldsymbol{\beta}}) = \boldsymbol{\beta} \hspace{10mm} Var(\hat{\boldsymbol{\beta}}) = \sigma^2_{\epsilon}(\boldsymbol{X}^T\boldsymbol{X})^{-1}
$$

## Sampling distribution of $\hat{\beta}_j$

$$
\hat{\boldsymbol{\beta}} \sim N(\boldsymbol{\beta}, \sigma^2_\epsilon(\mathbf{X}^T\mathbf{X})^{-1})
$$

Let $\mathbf{C} = (\mathbf{X}^T\mathbf{X})^{-1}$. Then, for each coefficient $\hat{\beta}_j$,

::: incremental
-   $E(\hat{\beta}_j) = \boldsymbol{\beta}_j$, the $j^{th}$ element of $\boldsymbol{\beta}$

-   $Var(\hat{\beta}_j) = \sigma^2_{\epsilon}C_{jj}$

-   $Cov(\hat{\beta}_i, \hat{\beta}_j) = \sigma^2_{\epsilon}C_{ij}$
:::

# Hypothesis test for $\beta_j$

## Steps for a hypothesis test

1.  State the null and alternative hypotheses.
2.  Calculate a test statistic.
3.  Calculate the p-value.
4.  State the conclusion.

## Hypothesis test for $\beta_j$: Hypotheses

We will generally test the hypotheses:

$$
\begin{aligned}
&H_0: \beta_j = 0 \\
&H_a: \beta_j \neq 0
\end{aligned}
$$

::: question
State these hypotheses in words.
:::

## Hypothesis test for $\beta_j$: Test statistic {.midi}

**Test statistic:** Number of standard errors the estimate is away from the null

$$
\text{Test Statstic} = \frac{\text{Estimate - Null}}{\text{Standard error}} \\
$$

. . .

If $\sigma^2_{\epsilon}$ was known, the test statistic would be

$$Z = \frac{\hat{\beta}_j - 0}{SE(\hat{\beta}_j)} ~ = ~\frac{\hat{\beta}_j - 0}{\sqrt{\sigma^2_\epsilon C_{jj}}} ~\sim ~ N(0, 1)
$$

. . .

In general, $\sigma^2_{\epsilon}$ is [not]{.underline} known, so we use $\hat{\sigma}_{\epsilon}^2$ to calculate $SE(\hat{\beta}_j)$

$$T = \frac{\hat{\beta}_j - 0}{SE(\hat{\beta}_j)} ~ = ~\frac{\hat{\beta}_j - 0}{\sqrt{\hat{\sigma}^2_\epsilon C_{jj}}} ~\sim ~ t_{n-p-1}
$$

## Hypothesis test for $\beta_j$: Test statistic

-   The test statistic $T$ follows a $t$ distribution with $n - p -1$ degrees of freedom.

-   We need to account for the additional variability introduced by calculating $SE(\hat{\beta}_j)$ using an estimated value instead of a constant

## *t* vs. N(0,1)

```{r}
#| label: fig-normal-t-curves
#| fig-cap: Standard normal vs. t distributions
#| echo: false

colors <- c("N(0,1)" = "black", 
            "t, df = 2" = "red", 
            "t, df = 5" = "blue",
            "t, df = 10" = "darkgreen", 
            "t, df = 30" = "purple")
ggplot() + 
  xlim(-5, 5) + 
  geom_function(fun = dnorm, aes(color = "N(0,1)")) + 
  geom_function(fun = dt,args = list(df = 2), aes(color = "t, df = 2")) +
  geom_function(fun = dt,args = list(df = 5), aes(color = "t, df = 5")) + 
  geom_function(fun = dt,args = list(df = 10), aes(color = "t, df = 10"))  +
  geom_function(fun = dt,args = list(df = 30), aes(color ="t, df = 30")) + 
  scale_color_manual(values = colors) +
    labs(x = "", y = "", color = "") + 
  theme_bw()
```

## Hypothesis test for $\beta_j$: P-value

The **p-value** is the probability of observing a test statistic at least as extreme (in the direction of the alternative hypothesis) from the null value as the one observed

$$
p-value = P(|t| > |\text{test statistic}|),
$$

calculated from a $t$ distribution with $n- p - 1$ degrees of freedom

. . .

::: question
Why do we take into account "extreme" on both the high and low ends?
:::

## Understanding the p-value

| Magnitude of p-value    | Interpretation                        |
|:------------------------|:--------------------------------------|
| p-value \< 0.01         | strong evidence against $H_0$         |
| 0.01 \< p-value \< 0.05 | moderate evidence against $H_0$       |
| 0.05 \< p-value \< 0.1  | weak evidence against $H_0$           |
| p-value \> 0.1          | effectively no evidence against $H_0$ |

**These are general guidelines. The strength of evidence depends on the context of the problem.**

## Hypothesis test for $\beta_j$: Conclusion

**There are two parts to the conclusion**

-   Make a conclusion by comparing the p-value to a predetermined decision-making threshold called the significance level ( $\alpha$ level)

    -   If $\text{P-value} < \alpha$: Reject $H_0$

    -   If $\text{P-value} \geq \alpha$: Fal to reject $H_0$

-   State the conclusion in the context of the data

# Application exercise

::: appex
ðŸ“‹ <https://sta221-fa24.netlify.app/ae/ae-03-inference>
:::

# Confidence interval for $\beta_j$

## Confidence interval for $\beta_j$

::: incremental
-   A plausible range of values for a population parameter is called a **confidence interval**

-   Using only a single point estimate is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net

    -   We can throw a spear where we saw a fish but we will probably miss, if we toss a net in that area, we have a good chance of catching the fish

    -   Similarly, if we report a point estimate, we probably will not hit the exact population parameter, but if we report a range of plausible values we have a good shot at capturing the parameter
:::

## What "confidence" means

::: incremental
-   We will construct $C\%$ confidence intervals.

    -   The confidence level impacts the width of the interval

-   "Confident" means if we were to take repeated samples of the same size as our data, fit regression lines using the same predictors, and calculate $C\%$ CIs for the coefficient of $x_j$, then $C\%$ of those intervals will contain the true value of the coefficient $\beta_j$

-   Balance precision and accuracy when selecting a confidence level
:::

## Confidence interval for $\beta_j$

$$
\text{Estimate} \pm \text{ (critical value) } \times \text{SE}
$$

. . .

$$
\hat{\beta}_1 \pm t^* \times SE({\hat{\beta}_j})
$$

where $t^*$ is calculated from a $t$ distribution with $n-p-1$ degrees of freedom

## Confidence interval: Critical value

::: columns
::: {.column width="60%"}
::: {.fragment fragment-index="1"}
```{r}
#| echo: true

# confidence level: 95%
qt(0.975, df = nrow(football) - 2 - 1)
```
:::

::: {.fragment fragment-index="2"}
```{r}
# confidence level: 90%
qt(0.95, df = nrow(football) - 2 - 1)
```
:::

::: {.fragment fragment-index="3"}
```{r}
# confidence level: 99%
qt(0.995, df = nrow(football) - 2 - 1)
```
:::
:::

::: {.column width="40%"}
:::
:::

## 95% CI for $\beta_j$: Calculation

```{r}
#| echo: false
tidy(exp_fit) |> 
  kable(digits = 3)
```

## 95% CI for $\beta_j$ in R

```{r}
#| echo: true

tidy(exp_fit, conf.int = TRUE, conf.level = 0.95) |> 
  kable(digits = 3)
```

<br>

**Interpretation**: We are 95% confident that for each additional 1,000 students enrolled, the institution's expenditures on football will be greater by \$598,000 to \$963,000, on average, holding institution type constant.

# Test for overall significance

## Test for overall significance: Hypotheses

We can conduct a hypothesis test using the ANOVA table to determine if there is at least one non-zero coefficient in the model

$$
\begin{aligned}
&H_0: \beta_1 = \dots = \beta_p = 0\\
&H_a: \beta_j \neq 0 \text{ for at least one }j 
\end{aligned}
$$

. . .

**For the football data**

$$
\begin{aligned}
&H_0: \beta_1 = \beta_2 = 0\\
&H_a: \beta_j \neq 0 \text{ for at least one }j 
\end{aligned}
$$

## Test for overall significance: Test statistic {.midi}

```{r}
#| echo: false

football_anova <- anova(lm(total_exp_m ~ enrollment_th + type, data = football)) |>
  tidy() |>
  mutate(term = if_else(term %in% c("enrollment_th", "type"), "Model", term))

ssm <- round(football_anova$sumsq[1] + football_anova$sumsq[2], 3)
dfm <- football_anova$df[1] + football_anova$df[2]
msm <- ssm / dfm
ssr <- round(football_anova$sumsq[3], 3)
sst <- ssm + ssr
stat <- msm / football_anova$meansq[3]


#replace model row with total model values 
football_anova$df[1] = dfm
football_anova$sumsq[1] = ssm
football_anova$meansq[1] = msm
football_anova$statistic[1] = stat
football_anova$p.value[1] = pf(stat, dfm, football_anova$df[3], lower.tail = FALSE)


football_anova <- football_anova |>
  slice(-2) |> # remove extra row for model variable
  add_row(term = "Total", df = nrow(football) - 1, sumsq = sst, meansq = NA, statistic = NA, p.value = NA) 

football_anova <- football_anova |>
  mutate(meansq = round(meansq,3), 
         statistic = round(statistic, 3), 
         p.value = round(p.value, 3), 
         meansq = as.character(meansq), 
         statistic = as.character(statistic), 
         p.value = as.character(p.value),
         meansq = if_else(is.na(meansq), "", meansq), 
         statistic = if_else(is.na(statistic), "", statistic), 
         p.value = if_else(is.na(p.value), "", p.value))

football_anova <- football_anova |>
  rename("Source" = term, 
         "Df" = df,
         "Sum Sq" = sumsq, 
         "Mean Sq" = meansq, 
         "F Stat" = statistic, 
         "Pr(> F)" = p.value) 



football_anova |>
  kable(digits = 3)
```

<br>

**Test statistic**: Ratio of explained to unexplained variability

$$
F = \frac{\text{Mean Square Model}}{\text{Mean Square Residuals}}
$$

The test statistic follows an $F$ distribution with $p$ and $n -  p - 1$ degrees of freedom

## Test for overall significance: P-value

```{r}
#| echo: false

## F distribution 
ggplot() +
  stat_function(fun = df, 
                geom = "line", 
                args = list(df1 = 2, df2 = 124), 
                color = "steelblue2", lwd = 2, 
                xlim = c(0, 6)) +
  labs(x = "F", 
       y = "density", 
       title = "F distribution with 2 and 124 df") + 
  theme_classic()
```

$$
\text{P-value} = \text{Pr}(F > \text{F Stat})
$$

## Test for overall significance: Conclusion

**There are two parts to the conclusion**

-   Make a conclusion by comparing the p-value to a predetermined decision-making threshold called the significance level ( $\alpha$ level)

    -   If $\text{P-value} < \alpha$: Reject $H_0$

    -   If $\text{P-value} \geq \alpha$: Fal to reject $H_0$

-   State the conclusion in the context of the data.

## Test for overall significance: Conclusion {.midi}

$$
\begin{aligned}
&H_0: \beta_1 = \beta_2 = 0\\
&H_a: \beta_j \neq 0 \text{ for at least one }j 
\end{aligned}
$$

```{r}
football_anova |>
  kable(digits = 3)
```

::: question
What is the conclusion from this hypothesis test?
:::

## Recap

-   Introduced statistical inference in the context of regression

-   Described the assumptions for regression

-   Connected the distribution of residuals and inferential procedures

-   Conducted inference on a single coefficient

-   Conducted inference on the overall regression model
